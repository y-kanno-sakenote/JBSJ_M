# modules/analysis/keywords.py
# -*- coding: utf-8 -*-
"""
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã‚¿ãƒ–ï¼ˆå®Œæˆç‰ˆãƒ»å®‰å…¨ãªé…å»¶å®Ÿè¡Œï¼†ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‹ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œï¼‰

æ©Ÿèƒ½ï¼ˆå¾“æ¥ã©ãŠã‚Šï¼‰:
â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
   - å¹´ãƒ»å¯¾è±¡ç‰©ãƒ»ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§çµã‚Šè¾¼ã¿
   - å‡ºç¾å›æ•°ä¸Šä½ã‚’ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆè¡¨ç¤º
   - WordCloudï¼ˆwordcloud ãŒã‚ã‚Œã°ï¼‰ã‚’ä»»æ„è¡¨ç¤ºï¼ˆæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆå¯¾å¿œï¼‰

â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆé‡ã„ã®ã§é…å»¶æç”»ï¼‰
   - åŒä¸€è«–æ–‡å†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…±èµ·ã‚’ networkx + pyvis ã§å¯è¦–åŒ–
   - ã€Œãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æç”»ã€ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã®ã¿ç”Ÿæˆ
   - ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆmodules/common/cache_utils.pyï¼‰å¯¾å¿œ

â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆçµŒå¹´å¤‰åŒ–ï¼‰
   - å¹´ã”ã¨ã«å‡ºç¾é »åº¦ã‚’é›†è¨ˆã—ã€TopNèªã‚’æŠ˜ã‚Œç·šã§å¯è¦–åŒ–ï¼ˆPlotlyãŒãªã‘ã‚Œã°st.line_chartï¼‰

æ³¨æ„ï¼š
- importæ™‚ã«é‡ã„å‡¦ç†ã‚’ä¸€åˆ‡èµ°ã‚‰ã›ã¾ã›ã‚“ï¼ˆé–¢æ•°å†…ã®ã¿ã§å®Ÿè¡Œï¼‰
- ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆkeyã¯ "kw_*" æ¥é ­ã§ä»–ã‚¿ãƒ–ã¨è¡çªã—ãªã„ã‚ˆã†ã«ã—ã¦ã„ã¾ã™
"""

from __future__ import annotations
import re
from typing import List, Tuple, Dict, Any

import pandas as pd
import streamlit as st
from pathlib import Path

# ä¸¦ã³é †ï¼ˆè¡¨ç¤ºé †ï¼‰ã‚’å›ºå®šã™ã‚‹ãŸã‚ã®å®šæ•°
TARGET_ORDER = [
    "æ¸…é…’","ãƒ“ãƒ¼ãƒ«","ãƒ¯ã‚¤ãƒ³","ç„¼é…","ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«é£²æ–™","ç™ºé…µä¹³ãƒ»ä¹³è£½å“",
    "é†¤æ²¹","å‘³å™Œ","ç™ºé…µé£Ÿå“","è¾²ç”£ç‰©ãƒ»æœå®Ÿ","å‰¯ç”£ç‰©ãƒ»ãƒã‚¤ã‚ªãƒã‚¹","é…µæ¯ãƒ»å¾®ç”Ÿç‰©","ã‚¢ãƒŸãƒé…¸ãƒ»ã‚¿ãƒ³ãƒ‘ã‚¯è³ª","ãã®ä»–"
]
TYPE_ORDER = [
    "å¾®ç”Ÿç‰©ãƒ»éºä¼å­é–¢é€£","é†¸é€ å·¥ç¨‹ãƒ»è£½é€ æŠ€è¡“","å¿œç”¨åˆ©ç”¨ãƒ»é£Ÿå“é–‹ç™º","æˆåˆ†åˆ†æãƒ»ç‰©æ€§è©•ä¾¡",
    "å“è³ªè©•ä¾¡ãƒ»å®˜èƒ½è©•ä¾¡","æ­´å²ãƒ»æ–‡åŒ–ãƒ»çµŒæ¸ˆ","å¥åº·æ©Ÿèƒ½ãƒ»æ „é¤ŠåŠ¹æœ","çµ±è¨ˆè§£æãƒ»ãƒ¢ãƒ‡ãƒ«åŒ–",
    "ç’°å¢ƒãƒ»ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£","ä¿å­˜ãƒ»å®‰å®šæ€§","ãã®ä»–ï¼ˆç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼‰"
]

def _order_options(all_options: list[str], preferred: list[str]) -> list[str]:
    """preferred ã«å«ã¾ã‚Œã‚‹ã‚‚ã®ã¯ãã®é †ã§å…ˆé ­ã«ã€ãã‚Œä»¥å¤–ã¯äº”åéŸ³ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆï¼‰é †ã§å¾Œã‚ã«ä¸¦ã¹ã‚‹"""
    s = set(all_options)
    head = [x for x in preferred if x in s]
    tail = sorted([x for x in all_options if x not in preferred])
    return head + tail

# --- è¿½åŠ : ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¨ãƒã‚¤ã‚ºåˆ¤å®š ---
try:
    from wordcloud import STOPWORDS as WC_STOPWORDS  # type: ignore
    _WC = set(x.casefold() for x in WC_STOPWORDS)
except Exception:
    _WC = set()

STOPWORDS_EN_EXTRA = {
    "and","the","of","to","in","on","for","with","was","were","is","are","be","by","at","from",
    "as","that","this","these","those","an","a","it","its","we","our","you","your","can","may",
    "also","using","use","used","based","between","within","into","than","over","after","before",
    "such","fig","figure","fig.", "table","et","al","etc",
}

STOPWORDS_JA = {
    "ã“ã¨","ã‚‚ã®","ãŸã‚","ãªã©","ã‚ˆã†","å ´åˆ","ãŠã‚ˆã³","åŠã³","ã¾ãŸ","ã“ã‚Œ","ãã‚Œ","ã“ã®","ãã®",
    "å›³","è¡¨","ç¬¬","åŒ","ä¸€æ–¹","ã¾ãŸã¯","åˆã¯","ã«ãŠã‘ã‚‹","ã«ã¤ã„ã¦","ã«å¯¾ã™ã‚‹"
}

STOPWORDS_ALL = _WC | {s.casefold() for s in STOPWORDS_EN_EXTRA} | STOPWORDS_JA

_PUNCT_EDGE_RE = re.compile(r"^[\W_]+|[\W_]+$")   # å‰å¾Œã®è¨˜å·ã‚’å‰¥ãŒã™
_NUM_RE        = re.compile(r"^\d+(\.\d+)?$")     # æ•°å­—ã®ã¿
_EN_SHORT_RE   = re.compile(r"^[A-Za-z]{1,2}$")   # 1â€“2æ–‡å­—ã®è‹±å­—ï¼ˆçŸ­ã™ãï¼‰

def _clean_token(tok: str) -> str:
    if tok is None:
        return ""
    t = str(tok).strip()
    if not t:
        return ""
    # å‰å¾Œã®è¨˜å·ã‚’é™¤å»
    t = _PUNCT_EDGE_RE.sub("", t)
    if not t:
        return ""
    low = t.casefold()
    if low in {"none", "nan"}:
        return ""
    if _NUM_RE.fullmatch(t):
        return ""
    if _EN_SHORT_RE.fullmatch(t):
        return ""
    if low in STOPWORDS_ALL:
        return ""
    return t

def _get_japanese_font_path() -> str | None:
    """æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåŒæ¢±ã‚’æœ€å„ªå…ˆã€‚"""
    candidates = [
        "fonts/IPAexGothic.ttf",                            # â† åŒæ¢±æ¨å¥¨
        "/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf",
        "/usr/share/fonts/opentype/ipafont-mincho/ipam.ttf",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc",      # mac
    ]
    for p in candidates:
        if Path(p).exists():
            return p
    return None

# ==== Optional depsï¼ˆç„¡ãã¦ã‚‚å‹•ãï¼‰ ====
try:
    import plotly.express as px  # type: ignore
    HAS_PX = True
except Exception:
    HAS_PX = False

try:
    from wordcloud import WordCloud  # type: ignore
    HAS_WC = True
except Exception:
    HAS_WC = False

try:
    import networkx as nx  # type: ignore
    HAS_NX = True
except Exception:
    HAS_NX = False

try:
    from pyvis.network import Network  # type: ignore
    HAS_PYVIS = True
except Exception:
    HAS_PYVIS = False

# æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥IOï¼ˆã‚ã‚Œã°ä½¿ã†ï¼‰
try:
    from modules.common.cache_utils import cache_csv_path, load_csv_if_exists, save_csv
    HAS_DISK_CACHE = True
except Exception:
    HAS_DISK_CACHE = False


# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
_SPLIT_MULTI_RE = re.compile(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+")

KEY_COLS = [
    "llm_keywords","primary_keywords","secondary_keywords","featured_keywords",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰4","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰5",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰6","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰7","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰8","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰9","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰10",
]

# ---- ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆè‹±èªï¼‹æ—¥æœ¬èªã®æ±ç”¨ãƒã‚¤ã‚º + 'nan'ï¼‰----
STOPWORDS = set([
    # è‹±èªç³»
    "and","the","of","to","in","for","on","at","with","by","an","is","are",
    "this","that","it","as","be","from","was","were","or","a","we","our",
    "their","can","may","will","using","use","used","study","based",
    "analysis","data","result","results","method","methods","conclusion",
    "discussion","introduction","materials","material","supplementary",
    "figure","table","et","al","etc","between","among","within","into",
    "over","under","than","then","there","here","such","these","those",
    "however","therefore","thus","because","due","per","based","according",
    "observed","obtained","present","presented","approach","paper","research",
    "nan","none","null",
    # æ—¥æœ¬èªç³»ï¼ˆåŠ©è©ãƒ»å½¢å¼åè©ãƒ»æ±ç”¨ãƒã‚¤ã‚ºï¼‰
    "ã“ã‚Œ","ãã‚Œ","ã‚ã‚Œ","ãŸã‚","ã‚‚ã®","ã“ã¨","ã‚ˆã†","ã¾ãŸ","ãŠã‚ˆã³","ãŠã‚ˆã³ã³",
    "ã«ãŠã‘ã‚‹","ã«ã¤ã„ã¦","ã«ã‚ˆã‚Š","ã«ã‚ˆã‚‹","ãªã©","ã™ã‚‹","ã—ãŸ","ã—ã¦","ã•ã‚Œ","ã•ã‚Œã‚‹",
    "ã„ã‚‹","ã‚ã‚‹","ãªã‚‹","ã§ãã‚‹","å¯èƒ½","çµæœ","æ–¹æ³•","ç›®çš„","è€ƒå¯Ÿ","çµè«–","åºè«–",
    "å›³","è¡¨","ä¾‹","ä¾‹ãˆã°","æœ¬ç ”ç©¶","æœ¬è«–æ–‡","æœ¬å ±","æœ¬å ±å‘Š","æœ¬ç¨¿","ä¸€æ–¹","ä¸€æ–¹ã§",
    "ã•ã‚‰ã«","ã—ã‹ã—","ãã“ã§","ã¾ãš","æ¬¡ã«","æœ€å¾Œ","ä»¥ä¸Š","ä»¥ä¸‹","æœ¬","å„","æœ¬å­¦","åŒ",
])

def norm_key(s: str) -> str:
    s = str(s or "").replace("\u00A0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s.lower()

def split_multi(s) -> List[str]:
    if not s:
        return []
    return [w.strip() for w in _SPLIT_MULTI_RE.split(str(s)) if w.strip()]

def col_contains_any(df_col: pd.Series, needles: List[str]) -> pd.Series:
    if not needles:
        return pd.Series([True] * len(df_col), index=df_col.index)
    lo_needles = [norm_key(n) for n in needles]
    def _hit(v: str) -> bool:
        s = norm_key(v)
        return any(n in s for n in lo_needles)
    return df_col.fillna("").astype(str).map(_hit)

@st.cache_data(ttl=600, show_spinner=False)
def year_min_max(df: pd.DataFrame) -> Tuple[int, int]:
    if "ç™ºè¡Œå¹´" not in df.columns:
        return (1980, 2025)
    y = pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce")
    if y.notna().any():
        return (int(y.min()), int(y.max()))
    return (1980, 2025)

def _apply_filters(df: pd.DataFrame,
                   y_from: int, y_to: int,
                   targets: List[str], types: List[str]) -> pd.DataFrame:
    use = df.copy()
    if "ç™ºè¡Œå¹´" in use.columns:
        y = pd.to_numeric(use["ç™ºè¡Œå¹´"], errors="coerce")
        use = use[(y >= y_from) & (y <= y_to) | y.isna()]
    if targets and "å¯¾è±¡ç‰©_top3" in use.columns:
        use = use[col_contains_any(use["å¯¾è±¡ç‰©_top3"], targets)]
    if types and "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in use.columns:
        use = use[col_contains_any(use["ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3"], types)]
    return use

def _extract_keywords_from_row(row: pd.Series) -> List[str]:
    words: List[str] = []
    for c in KEY_COLS:
        if c in row and pd.notna(row[c]):
            for w in split_multi(row[c]):
                cw = _clean_token(w)
                if cw:
                    words.append(cw)
    return words

@st.cache_data(ttl=600, show_spinner=False)
def collect_keywords(df: pd.DataFrame) -> pd.Series:
    """å…¨è¡Œã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã‚’æŠ½å‡ºã—ã¦1æœ¬ã®Seriesã«"""
    bags: List[str] = []
    for _, r in df.iterrows():
        bags += _extract_keywords_from_row(r)
    return pd.Series(bags, dtype="object")

@st.cache_data(ttl=600, show_spinner=False)
def keyword_freq(df: pd.DataFrame) -> pd.Series:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é »åº¦ï¼ˆé™é †ï¼‰"""
    s = collect_keywords(df)
    if s.empty:
        return pd.Series(dtype=int)
    return s.value_counts().sort_values(ascending=False)

@st.cache_data(ttl=600, show_spinner=False)
def yearly_keyword_counts(df: pd.DataFrame) -> pd.DataFrame:
    """å¹´Ã—èªã®ä»¶æ•°ï¼ˆè«–æ–‡ã”ã¨é‡è¤‡é™¤å»ï¼‰"""
    if "ç™ºè¡Œå¹´" not in df.columns:
        return pd.DataFrame(columns=["ç™ºè¡Œå¹´", "keyword", "count"])
    rows = []
    for _, r in df.iterrows():
        y = pd.to_numeric(r.get("ç™ºè¡Œå¹´"), errors="coerce")
        if pd.isna(y): 
            continue
        kws = list(dict.fromkeys(_extract_keywords_from_row(r)))
        for k in kws:
            rows.append((int(y), k))
    if not rows:
        return pd.DataFrame(columns=["ç™ºè¡Œå¹´", "keyword", "count"])
    c = pd.DataFrame(rows, columns=["ç™ºè¡Œå¹´","keyword"]).value_counts().reset_index(name="count")
    return c.sort_values(["ç™ºè¡Œå¹´","count"], ascending=[True, False]).reset_index(drop=True)

# ====== å…±èµ·ã‚¨ãƒƒã‚¸ï¼ˆé‡ã„ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰ ======
@st.cache_data(ttl=600, show_spinner=False)
def build_keyword_cooccur_edges(df: pd.DataFrame, min_edge: int) -> pd.DataFrame:
    """
    åŒä¸€è«–æ–‡å†…ã§å…±èµ·ã™ã‚‹èªã®ãƒšã‚¢ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    æˆ»ã‚Šå€¤: ['src','dst','weight']
    """
    rows = []
    for _, r in df.iterrows():
        kws = sorted(set(_extract_keywords_from_row(r)))
        # å…¨çµ„åˆã›
        for i in range(len(kws)):
            for j in range(i+1, len(kws)):
                rows.append((kws[i], kws[j]))
    if not rows:
        return pd.DataFrame(columns=["src","dst","weight"])
    edges = pd.DataFrame(rows, columns=["src","dst"]).value_counts().reset_index(name="weight")
    edges = edges[edges["weight"] >= int(min_edge)].sort_values("weight", ascending=False).reset_index(drop=True)
    return edges

def _freq_to_df(freq: pd.Series, topn: int) -> pd.DataFrame:
    if freq.empty:
        return pd.DataFrame(columns=["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰","ä»¶æ•°"])
    df = freq.head(int(topn)).reset_index()
    df.columns = ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰","ä»¶æ•°"]
    return df

def _draw_pyvis_from_edges(edges: pd.DataFrame, height_px: int = 650) -> None:
    if not (HAS_NX and HAS_PYVIS):
        st.info("networkx / pyvis ãŒæœªå°å…¥ã®ãŸã‚ã€è¡¨ã®ã¿è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
        return
    if edges.empty:
        st.warning("å¯¾è±¡æ¡ä»¶ã§ã‚¨ãƒƒã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # æ–‡å­—åˆ—IDã«çµ±ä¸€
    G = nx.Graph()
    for _, r in edges.iterrows():
        s = str(r["src"]); t = str(r["dst"]); w = int(r["weight"])
        if G.has_edge(s, t):
            G[s][t]["weight"] += w
        else:
            G.add_edge(s, t, weight=w)

    net = Network(height=f"{height_px}px", width="100%", bgcolor="#ffffff", font_color="#222")
    net.barnes_hut(gravity=-30000, central_gravity=0.25, spring_length=120, spring_strength=0.02)
    net.from_nx(G)
    html = net.generate_html(notebook=False)  # â† ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•ã‚ªãƒ¼ãƒ—ãƒ³å›é¿
    st.components.v1.html(html, height=height_px, scrolling=True)

# ==== è¿½åŠ ï¼šå®‰å…¨è¡¨ç¤ºãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼ˆUIã¯å¤‰ãˆãšã«è½ã¡ã«ããï¼‰ ====
def safe_show_image(obj: Any) -> None:
    """
    Streamlitã®ç”»åƒè¡¨ç¤ºã§å‹å·®ç•°ãŒã‚ã£ã¦ã‚‚è½ã¡ãªã„ã‚ˆã†ã«å®‰å…¨ã«è¡¨ç¤ºã™ã‚‹ã€‚
    UIï¼ˆæç”»çµæœï¼‰ã¯å¤‰æ›´ã—ãªã„ã€‚
    """
    import numpy as np
    import io
    try:
        from PIL import Image
    except Exception:
        Image = None  # type: ignore

    # None
    if obj is None:
        st.warning("ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒ None ã§ã—ãŸã€‚ç”Ÿæˆã«å¤±æ•—ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return

    # Matplotlib Figure -> pyplot
    try:
        import matplotlib.figure
        if isinstance(obj, matplotlib.figure.Figure):
            st.pyplot(obj)
            return
    except Exception:
        pass

    # PIL.Image ã¯å¿…ãš PNG ãƒã‚¤ãƒˆåˆ—ã¸å¤‰æ›ã—ã¦ã‹ã‚‰è¡¨ç¤ºï¼ˆç’°å¢ƒå·®å¯¾ç­–ï¼‰
    if Image is not None and isinstance(obj, Image.Image):
        try:
            img = obj
            # é€éã‚„ãƒ‘ãƒ¬ãƒƒãƒˆç­‰ã®ãƒ¢ãƒ¼ãƒ‰ã‚’çµ±ä¸€
            if img.mode not in ("RGB", "RGBA"):
                # ãƒ‘ãƒ¬ãƒƒãƒˆã‚„F/LAãªã©ã¯RGBAåŒ–ãŒå®‰å…¨
                img = img.convert("RGBA")
            buf = io.BytesIO()
            img.save(buf, format="PNG")  # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯PNGå›ºå®šã§å®‰å®š
            st.image(buf.getvalue(), use_container_width=True)
        except Exception as e:
            st.warning(f"PILç”»åƒã®è¡¨ç¤ºã§ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e!s}")
        return

    # NumPy array
    if isinstance(obj, np.ndarray):
        arr = obj
        # å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
        if arr.ndim == 2:
            pass  # gray OK
        elif arr.ndim == 3 and arr.shape[2] in (3, 4):
            pass
        else:
            st.warning(f"æƒ³å®šå¤–ã®é…åˆ—shapeã§ã™: {arr.shape}")
            return
        # dtypeã‚’uint8ã¸
        if arr.dtype in (np.float32, np.float64):
            a = arr
            if np.nanmax(a) <= 1.0:
                a = (np.nan_to_num(a) * 255.0).clip(0, 255).astype(np.uint8)
            else:
                a = np.nan_to_num(a).clip(0, 255).astype(np.uint8)
            st.image(a, use_container_width=True)
        elif arr.dtype == np.uint8:
            st.image(arr, use_container_width=True)
        else:
            a = np.nan_to_num(arr).clip(0, 255).astype(np.uint8)
            st.image(a, use_container_width=True)
        return

    # bytes / bytearray
    if isinstance(obj, (bytes, bytearray)):
        st.image(obj, use_container_width=True)
        return

    # æ–‡å­—åˆ—ï¼ˆURL or ãƒ‘ã‚¹ï¼‰
    if isinstance(obj, str):
        st.image(obj, use_container_width=True)
        return

    # ãã‚Œä»¥å¤–
    st.warning(f"st.imageãŒæ‰±ãˆãªã„å‹ã§ã—ãŸ: {type(obj)}")
    
# ========= â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ =========
def _render_freq_block(df: pd.DataFrame) -> None:
    st.markdown("### â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")

    ymin, ymax = year_min_max(df)
    c1, c2, c3, c4 = st.columns([1,1,1,1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax,
                                 value=(ymin, ymax), key="kw_freq_year")

    # â–¼ å€™è£œãƒªã‚¹ãƒˆã‚’è‡ªå‹•æŠ½å‡º
    targets_all = sorted({w for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("")
                          for w in _SPLIT_MULTI_RE.split(v) if w.strip()})
    types_all   = sorted({w for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("")
                          for w in _SPLIT_MULTI_RE.split(v) if w.strip()})

    # â˜… è¡¨ç¤ºé †ã‚’å›ºå®š
    targets_all = _order_options(targets_all, TARGET_ORDER)
    types_all   = _order_options(types_all, TYPE_ORDER)

    with c2:
        tg_needles = st.multiselect("å¯¾è±¡ç‰©ã§çµã‚Šè¾¼ã¿", options=targets_all, default=[], key="kw_freq_tg")
    with c3:
        tp_needles = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§çµã‚Šè¾¼ã¿", options=types_all, default=[], key="kw_freq_tp")
    with c4:
        topn = st.number_input("è¡¨ç¤ºä»¶æ•°", min_value=5, max_value=100, value=30, step=5, key="kw_freq_topn")

    # â–¼ ãƒ•ã‚£ãƒ«ã‚¿åæ˜ 
    use = _apply_filters(df, y_from, y_to, tg_needles, tp_needles)
    freq = keyword_freq(use)
    freq_df = _freq_to_df(freq, int(topn))

    if freq_df.empty:
        st.info("æ¡ä»¶ã«åˆã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # ã‚°ãƒ©ãƒ•
    if HAS_PX:
        fig = px.bar(freq_df, x="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", y="ä»¶æ•°", text_auto=True, title="é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸Šä½ï¼‰")
        fig.update_layout(margin=dict(l=10,r=10,t=40,b=10), height=420)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.bar_chart(freq_df.set_index("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")["ä»¶æ•°"])

    # WordCloudï¼ˆä»»æ„ãƒ»ãƒœã‚¿ãƒ³ç”Ÿæˆï¼‰
    with st.expander("â˜ WordCloudï¼ˆä»»æ„ï¼‰", expanded=False):
        if HAS_WC:
            if st.button("ç”Ÿæˆã™ã‚‹", key="kw_wc_btn"):
                textfreq = {row["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]: int(row["ä»¶æ•°"]) for _, row in freq_df.iterrows()}
                # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆå¯¾å¿œï¼ˆè¦‹ã¤ã‹ã‚Œã°é©ç”¨ï¼‰
                font_path = _get_japanese_font_path()
                wc = WordCloud(width=900, height=450, background_color="white",
                               collocations=False, prefer_horizontal=1.0,
                               font_path=font_path or None)
                img = wc.generate_from_frequencies(textfreq).to_image()
                # --- ã“ã“ã ã‘å·®ã—æ›¿ãˆï¼ˆUIå¤‰æ›´ãªã—ï¼‰ ---
                safe_show_image(img)
        else:
            st.caption("â€» wordcloud ãŒæœªå°å…¥ã®ãŸã‚éè¡¨ç¤ºã§ã™ã€‚")

# ========= â‘¡ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆé…å»¶æç”»ï¼‰ =========
def _render_cooccur_block(df: pd.DataFrame) -> None:
    st.markdown("### â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")

    ymin, ymax = year_min_max(df)
    c1, c2, c3, c4 = st.columns([1,1,1,1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax,
                                 value=(ymin, ymax), key="kw_co_year")
    with c2:
        min_edge = st.number_input("ã‚¨ãƒƒã‚¸æœ€å°å›æ•° (wâ‰¥)", min_value=1, max_value=50, value=3, step=1, key="kw_co_minw")
    with c3:
        topN = st.number_input("ãƒãƒ¼ãƒ‰ä¸Šé™ï¼ˆå‡ºç¾ä¸Šä½ï¼‰", min_value=30, max_value=300, value=120, step=10, key="kw_co_topn")
    with c4:
        st.caption("ä¸‹ã®ãƒœã‚¿ãƒ³ã§æç”»ã—ã¾ã™ã€‚")

    # â–¼ å€™è£œãƒªã‚¹ãƒˆã‚’è‡ªå‹•æŠ½å‡º
    targets_all = sorted({w for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("")
                          for w in _SPLIT_MULTI_RE.split(v) if w.strip()})
    types_all   = sorted({w for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("")
                          for w in _SPLIT_MULTI_RE.split(v) if w.strip()})

    # â˜… è¡¨ç¤ºé †ã‚’å›ºå®š
    targets_all = _order_options(targets_all, TARGET_ORDER)
    types_all   = _order_options(types_all, TYPE_ORDER)

    c5, c6 = st.columns([1,1])
    with c5:
        tg_needles = st.multiselect("å¯¾è±¡ç‰©ã§çµã‚Šè¾¼ã¿", options=targets_all, default=[], key="kw_co_tg")
    with c6:
        tp_needles = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§çµã‚Šè¾¼ã¿", options=types_all, default=[], key="kw_co_tp")

    # â–¼ ãƒ•ã‚£ãƒ«ã‚¿åæ˜ 
    use = _apply_filters(df, y_from, y_to, tg_needles, tp_needles)

    # --- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨æç”»ãƒ­ã‚¸ãƒƒã‚¯ã¯ãã®ã¾ã¾ ---
    cache_key = f"kwco|{y_from}-{y_to}|min{min_edge}|top{topN}|tg{','.join(tg_needles)}|tp{','.join(tp_needles)}"
    edges = build_keyword_cooccur_edges(use, int(min_edge))
    if not edges.empty and int(topN) > 0:
        deg = pd.concat([edges.groupby("src")["weight"].sum(),
                         edges.groupby("dst")["weight"].sum()], axis=1).fillna(0).sum(axis=1)
        keep_nodes = set(deg.sort_values(ascending=False).head(int(topN)).index.tolist())
        edges = edges[edges["src"].isin(keep_nodes) & edges["dst"].isin(keep_nodes)].reset_index(drop=True)

    st.caption(f"ã‚¨ãƒƒã‚¸æ•°: {len(edges)}")
    st.dataframe(edges.head(200), use_container_width=True, hide_index=True)

    with st.expander("ğŸ•¸ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æç”»ï¼ˆPyVis / ä»»æ„ä¾å­˜ï¼‰", expanded=False):
        if HAS_PYVIS and HAS_NX:
            if st.button("ğŸŒ æç”»ã™ã‚‹", key="kw_co_draw"):
                _draw_pyvis_from_edges(edges, height_px=680)
        else:
            st.info("networkx / pyvis ãŒæœªå°å…¥ã®ãŸã‚ã€è¡¨ã®ã¿è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
            
# ========= â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆçµŒå¹´å¤‰åŒ–ï¼‰ =========
def _render_trend_block(df: pd.DataFrame) -> None:
    st.markdown("### â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆçµŒå¹´å¤‰åŒ–ï¼‰")

    ymin, ymax = year_min_max(df)
    c1, c2, c3 = st.columns([1,1,1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax,
                                 value=(ymin, ymax), key="kw_trend_year")
    with c2:
        topn = st.number_input("è¡¨ç¤ºã™ã‚‹èªæ•°ï¼ˆTopNï¼‰", min_value=5, max_value=50, value=15, step=5, key="kw_trend_topn")
    with c3:
        ma = st.number_input("ç§»å‹•å¹³å‡ï¼ˆå¹´ï¼‰", min_value=1, max_value=7, value=1, step=1, key="kw_trend_ma")

    use = _apply_filters(df, y_from, y_to, [], [])
    yearly = yearly_keyword_counts(use)
    if yearly.empty:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # æœ€æ–°å¹´ä»˜è¿‘ã®TopNèªã‚’é¸ã¶ï¼ˆå…¨ä½“ä¸Šä½ã ã¨å‡¡ä¾‹ãŒå¤šã™ãã‚‹ãŸã‚ï¼‰
    latest_year = yearly["ç™ºè¡Œå¹´"].max()
    latest_top = (yearly[yearly["ç™ºè¡Œå¹´"] == latest_year]
                  .sort_values("count", ascending=False)["keyword"]
                  .head(int(topn)).tolist())
    piv = (yearly[yearly["keyword"].isin(latest_top)]
           .pivot_table(index="ç™ºè¡Œå¹´", columns="keyword", values="count", aggfunc="sum")
           .fillna(0).sort_index())

    if int(ma) > 1:
        piv = piv.rolling(window=int(ma), min_periods=1).mean()

    if HAS_PX:
        fig = px.line(piv.reset_index().melt(id_vars="ç™ºè¡Œå¹´", var_name="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", value_name="ä»¶æ•°"),
                      x="ç™ºè¡Œå¹´", y="ä»¶æ•°", color="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", markers=True)
        fig.update_layout(height=520, margin=dict(l=10,r=10,t=30,b=10))
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.line_chart(piv)

# ========= ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼šã‚¿ãƒ–æœ¬ä½“ =========
def render_keyword_tab(df: pd.DataFrame) -> None:
    st.markdown("## ğŸ§  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ")

    tab1, tab2, tab3 = st.tabs([
        "â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
        "â‘¡ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ",
    ])

    with tab1:
        _render_freq_block(df)

    with tab2:
        _render_cooccur_block(df)   # â† é…å»¶æç”»ï¼ˆãƒœã‚¿ãƒ³å¼ï¼‰

    with tab3:
        _render_trend_block(df)