# -*- coding: utf-8 -*-
"""
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã‚¿ãƒ–
â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆbar / wordcloudï¼‰
â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆè»½é‡åŒ–ï¼‹ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆçµŒå¹´å¤‰åŒ–ï¼‰
"""

from __future__ import annotations
import re
import itertools
from pathlib import Path
from typing import List, Dict, Tuple
import pandas as pd
import streamlit as st

# ==== ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ä¾å­˜ ====
try:
    import plotly.express as px
    HAS_PX = True
except Exception:
    HAS_PX = False

try:
    from wordcloud import WordCloud  # pip install wordcloud
    HAS_WC = True
except Exception:
    HAS_WC = False

try:
    import networkx as nx  # pip install networkx
    HAS_NX = True
except Exception:
    HAS_NX = False

try:
    from pyvis.network import Network  # pip install pyvis
    HAS_PYVIS = True
except Exception:
    HAS_PYVIS = False

# ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
try:
    from modules.common.cache_utils import cache_csv_path, load_csv_if_exists, save_csv
    HAS_DISK_CACHE = True
except Exception:
    HAS_DISK_CACHE = False


# ========= æ­£è¦åŒ–ãƒ»åˆ†å‰² =========
_SPLIT_RE = re.compile(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+")

def norm_key(s: str) -> str:
    s = str(s or "")
    s = s.replace("\u00A0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s.lower()

def split_multi(cell) -> List[str]:
    if not cell:
        return []
    return [w.strip() for w in _SPLIT_RE.split(str(cell)) if w.strip()]

def collect_keyword_columns(df: pd.DataFrame) -> List[str]:
    """ã‚ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã‚’ã€å­˜åœ¨ã™ã‚‹ã‚‚ã®ã ã‘è¿”ã™"""
    candidates = [
        "primary_keywords","secondary_keywords","featured_keywords","llm_keywords",
        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰4","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰5",
        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰6","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰7","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰8","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰9","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰10",
        "keywords","keyword","KW","kw",
    ]
    cols = [c for c in candidates if c in df.columns]
    # å¿µã®ãŸã‚è¿½åŠ ã§ "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" ã‚’å‰æ–¹ä¸€è‡´ã§æ‹¾ã†
    cols += [c for c in df.columns if str(c).startswith("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰") and c not in cols]
    # é‡è¤‡æ’é™¤
    cols = list(dict.fromkeys(cols))
    return cols

def filter_df(df: pd.DataFrame,
              year_from: int|None, year_to: int|None,
              targets: List[str], types: List[str]) -> pd.DataFrame:
    out = df.copy()
    if year_from is not None and year_to is not None and "ç™ºè¡Œå¹´" in out.columns:
        y = pd.to_numeric(out["ç™ºè¡Œå¹´"], errors="coerce")
        out = out[(y >= year_from) & (y <= year_to) | y.isna()]
    if targets and "å¯¾è±¡ç‰©_top3" in out.columns:
        tgt = [norm_key(t) for t in targets]
        out = out[out["å¯¾è±¡ç‰©_top3"].fillna("").astype(str).map(lambda v: any(t in norm_key(v) for t in tgt))]
    if types and "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in out.columns:
        tps = [norm_key(t) for t in types]
        out = out[out["ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3"].fillna("").astype(str).map(lambda v: any(t in norm_key(v) for t in tps))]
    return out


# ========= é »åº¦è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ =========
@st.cache_data(ttl=600, show_spinner=False)
def _freq_count_cached(df: pd.DataFrame, year_from: int, year_to: int,
                       targets: List[str], types: List[str]) -> pd.DataFrame:
    use = filter_df(df, year_from, year_to, targets, types)
    kw_cols = collect_keyword_columns(use)
    if not kw_cols:
        return pd.DataFrame(columns=["keyword", "freq"])

    counter: Dict[str, int] = {}
    for _, row in use[kw_cols].fillna("").iterrows():
        tokens = []
        for c in kw_cols:
            tokens += split_multi(row[c])
        # è¨˜è¿°ãŒé‡è¤‡ã—ã¦ã„ã¦ã‚‚ç´”ç²‹é »åº¦ã§ã‚«ã‚¦ãƒ³ãƒˆï¼ˆåŒä¸€è«–æ–‡å†…ã§åŒèªãŒè¤‡æ•°åˆ—ã«ã‚ã£ã¦ã‚‚ãã®åˆ†åŠ ç®—ï¼‰
        for t in tokens:
            k = norm_key(t)
            if not k:
                continue
            counter[k] = counter.get(k, 0) + 1

    if not counter:
        return pd.DataFrame(columns=["keyword", "freq"])
    freq = pd.DataFrame(sorted(counter.items(), key=lambda x: (-x[1], x[0])), columns=["keyword", "freq"])
    return freq


# ========= å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆè»½é‡ï¼‹ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ =========
def _cooccur_cache_key(version: str,
                       year_from: int, year_to: int,
                       targets: List[str], types: List[str],
                       topK: int, min_w: int) -> str:
    # æ–‡å­—åˆ—åŒ–ã—ã¦ç½²åï¼ˆcache_utilså´ã§ãƒãƒƒã‚·ãƒ¥åŒ–ï¼‰
    return f"v{version}|y{year_from}-{year_to}|tg{tuple(sorted(targets))}|tp{tuple(sorted(types))}|K{topK}|w{min_w}"

@st.cache_data(ttl=600, show_spinner=False)
def _cooccur_edges_cached(df: pd.DataFrame, year_from: int, year_to: int,
                          targets: List[str], types: List[str],
                          topK: int, min_w: int,
                          use_disk_cache: bool) -> pd.DataFrame:
    """
    æ‰‹é †ï¼š
      1. ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®é »å‡º TopK ã‚’é¸å®š
      2. ãã®èªé›†åˆã«é™å®šã—ã¦ã€åŒä¸€è¡Œå†…å…±èµ·ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
      3. min_w ä»¥ä¸Šã®ã¿æ®‹ã™
      4. ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆCSVï¼‰ã«ã‚‚ä¿å­˜/èª­è¾¼ï¼ˆä»»æ„ï¼‰
    """
    version = "1"  # å°†æ¥ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰ãˆãŸã‚‰ä¸Šã’ã‚‹
    sig = _cooccur_cache_key(version, year_from, year_to, targets, types, topK, min_w)

    # ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    if use_disk_cache and HAS_DISK_CACHE:
        cache_path = cache_csv_path("kw_cooccur", sig)
        cached = load_csv_if_exists(cache_path)
        if cached is not None:
            return cached

    # ä¸Šä½èªã‚’æŠ½å‡º
    freq = _freq_count_cached(df, year_from, year_to, targets, types)
    if freq.empty:
        return pd.DataFrame(columns=["src", "dst", "weight"])
    vocab = set(freq.head(int(topK))["keyword"].tolist())

    use = filter_df(df, year_from, year_to, targets, types)
    kw_cols = collect_keyword_columns(use)
    if not kw_cols:
        return pd.DataFrame(columns=["src", "dst", "weight"])

    pairs: Dict[Tuple[str, str], int] = {}
    for _, row in use[kw_cols].fillna("").iterrows():
        toks = []
        for c in kw_cols:
            toks += split_multi(row[c])
        toks = [norm_key(t) for t in toks if norm_key(t) in vocab]
        toks_uniq = sorted(set(toks))
        # 1æ–‡çŒ®å†…ã§ã®é‡è¤‡ã¯1å›ã«ï¼ˆåŒèªå¤šå‡ºã®åã‚Šã‚’æŠ‘ãˆã‚‹ï¼‰
        for a, b in itertools.combinations(toks_uniq, 2):
            key = (a, b) if a <= b else (b, a)
            pairs[key] = pairs.get(key, 0) + 1

    if not pairs:
        edges = pd.DataFrame(columns=["src", "dst", "weight"])
    else:
        edges = pd.DataFrame(
            [(a, b, w) for (a, b), w in pairs.items() if w >= int(min_w)],
            columns=["src", "dst", "weight"]
        ).sort_values("weight", ascending=False).reset_index(drop=True)

    # ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
    if use_disk_cache and HAS_DISK_CACHE:
        try:
            save_csv(edges, cache_path)  # type: ignore[arg-type]
        except Exception:
            pass

    return edges


# ========= ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæ™‚ç³»åˆ—é »åº¦ï¼‰ =========
@st.cache_data(ttl=600, show_spinner=False)
def _trend_timeseries_cached(df: pd.DataFrame,
                             y_from: int, y_to: int,
                             targets: List[str], types: List[str],
                             topN: int) -> pd.DataFrame:
    use = filter_df(df, y_from, y_to, targets, types)
    if "ç™ºè¡Œå¹´" not in use.columns:
        return pd.DataFrame(columns=["year", "keyword", "freq"])

    kw_cols = collect_keyword_columns(use)
    if not kw_cols:
        return pd.DataFrame(columns=["year", "keyword", "freq"])

    # ã¾ãšå…¨ä½“ä¸Šä½ topN ã‚’æ±ºã‚ã¦ã‹ã‚‰å¹´åˆ¥ã«ã‚«ã‚¦ãƒ³ãƒˆ
    freq_all = _freq_count_cached(df, y_from, y_to, targets, types)
    use_vocab = set(freq_all.head(int(topN * 2))["keyword"].tolist())  # å°‘ã—ä½™è£•ã‚’æŒãŸã›ã‚‹

    rows = []
    y = pd.to_numeric(use["ç™ºè¡Œå¹´"], errors="coerce")
    use = use.assign(_y=y)
    use = use[use["_y"].notna()].copy()
    for _, row in use[["_y"] + kw_cols].iterrows():
        yr = int(row["_y"])
        toks = []
        for c in kw_cols:
            toks += split_multi(row[c])
        toks = [norm_key(t) for t in toks if norm_key(t) in use_vocab]
        for t in toks:
            rows.append((yr, t))

    if not rows:
        return pd.DataFrame(columns=["year", "keyword", "freq"])

    ts = pd.DataFrame(rows, columns=["year", "keyword"])
    ts["freq"] = 1
    ts = (ts.groupby(["year", "keyword"])["freq"].sum()
                .reset_index()
                .sort_values(["keyword", "year"]))
    # å¯è¦–åŒ–ã§è¦‹ã‚„ã™ã„ã‚ˆã†ã« keyword ã”ã¨ã«æœ€å¤§é »åº¦ãŒé«˜ã„é †ã§ topN ã«çµã‚‹
    top_kw = (ts.groupby("keyword")["freq"].sum()
                .sort_values(ascending=False)
                .head(int(topN)).index.tolist())
    ts = ts[ts["keyword"].isin(top_kw)].copy()
    return ts


# ========= UIï¼šé »å‡º =========
def _render_freq_block(df: pd.DataFrame):
    st.subheader("â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
    # å¹´ãƒ¬ãƒ³ã‚¸
    if "ç™ºè¡Œå¹´" in df.columns:
        y = pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce")
        ymin, ymax = (int(y.min()), int(y.max())) if y.notna().any() else (1980, 2025)
    else:
        ymin, ymax = 1980, 2025

    c1, c2, c3 = st.columns([1.2, 1, 1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax, value=(ymin, ymax), key="kw_freq_year")
    with c2:
        topN = st.number_input("ä¸Šä½ä»¶æ•°", min_value=10, max_value=200, value=50, step=10, key="kw_freq_topn")
    with c3:
        show_wc = st.toggle("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’è¡¨ç¤º", value=False, key="kw_freq_wc")

    c4, c5 = st.columns([1, 1])
    with c4:
        tg_raw = {t for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
        tg_all = sorted(tg_raw)
        tg_sel = st.multiselect("å¯¾è±¡ç‰©ã§çµã‚Šè¾¼ã¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", tg_all, default=[], key="kw_freq_tg")
    with c5:
        tp_raw = {t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
        tp_all = sorted(tp_raw)
        tp_sel = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§çµã‚Šè¾¼ã¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", tp_all, default=[], key="kw_freq_tp")

    freq = _freq_count_cached(df, y_from, y_to, tg_sel, tp_sel)
    if freq.empty:
        st.info("æ¡ä»¶ã«åˆã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    top = freq.head(int(topN)).copy()
    st.dataframe(top.rename(columns={"keyword": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "freq": "å‡ºç¾æ•°"}),
                 use_container_width=True, hide_index=True)

    if HAS_PX:
        fig = px.bar(top.head(30), x="keyword", y="freq",
                     labels={"keyword": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "freq": "å‡ºç¾æ•°"})
        fig.update_layout(xaxis_tickangle=-40, height=420, margin=dict(l=10, r=10, t=30, b=10))
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.bar_chart(top.set_index("keyword").head(30))

    if show_wc:
        if not HAS_WC:
            st.warning("wordcloud ãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ã™ã€‚`pip install wordcloud`")
        else:
            # é »åº¦è¾æ›¸ã«ã—ã¦æç”»
            freqs = {r["keyword"]: int(r["freq"]) for _, r in top.iterrows()}
            wc = WordCloud(width=1000, height=400, background_color="white", font_path=None)
            img = wc.generate_from_frequencies(freqs).to_image()
            st.image(img, caption="Word Cloud", use_column_width=True)


# ========= UIï¼šå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ =========
def _render_cooccur_block(df: pd.DataFrame):
    st.subheader("â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")

    # å¹´ãƒ¬ãƒ³ã‚¸
    if "ç™ºè¡Œå¹´" in df.columns:
        y = pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce")
        ymin, ymax = (int(y.min()), int(y.max())) if y.notna().any() else (1980, 2025)
    else:
        ymin, ymax = 1980, 2025

    c1, c2, c3, c4 = st.columns([1.2, 1, 1, 1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax, value=(ymin, ymax), key="kw_net_year")
    with c2:
        topK = st.number_input("èªå½™ã®ä¸Šä½Kï¼ˆãƒãƒ¼ãƒ‰æ•°ã®ä¸Šé™ï¼‰", min_value=30, max_value=500, value=150, step=10, key="kw_net_topk")
    with c3:
        min_w = st.number_input("æœ€å°å…±èµ·å›æ•° (wâ‰¥)", min_value=1, max_value=20, value=2, step=1, key="kw_net_minw")
    with c4:
        use_disk_cache = st.toggle("ğŸ—ƒ ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã†", value=True, key="kw_net_disk")

    c5, c6 = st.columns([1, 1])
    with c5:
        tg_raw = {t for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
        tg_all = sorted(tg_raw)
        tg_sel = st.multiselect("å¯¾è±¡ç‰©ã§çµã‚Šè¾¼ã¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", tg_all, default=[], key="kw_net_tg")
    with c6:
        tp_raw = {t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
        tp_all = sorted(tp_raw)
        tp_sel = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§çµã‚Šè¾¼ã¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", tp_all, default=[], key="kw_net_tp")

    edges = _cooccur_edges_cached(df, y_from, y_to, tg_sel, tp_sel, int(topK), int(min_w), bool(use_disk_cache))

    if edges.empty:
        st.info("æ¡ä»¶ã«åˆã†å…±èµ·é–¢ä¿‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        return

    st.caption(f"ã‚¨ãƒƒã‚¸æ•°: {len(edges)}ï¼ˆwâ‰¥{int(min_w)}ï¼‰")
    st.dataframe(edges.head(50), use_container_width=True, hide_index=True)

    # PyVisã§å¯è¦–åŒ–ï¼ˆä¾å­˜ãŒã‚ã‚Œã°ï¼‰
    if HAS_NX and HAS_PYVIS:
        G = nx.Graph()
        for _, r in edges.iterrows():
            s, t, w = str(r["src"]), str(r["dst"]), int(r["weight"])
            G.add_edge(s, t, weight=w)

        net = Network(height="650px", width="100%", bgcolor="#fff", font_color="#222")
        net.barnes_hut(gravity=-30000, central_gravity=0.25, spring_length=110, spring_strength=0.02)
        net.from_nx(G)

        # ç›´æ¥ show() ã¯ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ã“ã†ã¨ã™ã‚‹ã®ã§ã€generate_html() ç›¸å½“ã®æµå„€ã§åŸ‹ã‚è¾¼ã¿
        html_path = Path("kw_cooccur_network.html")
        net.write_html(str(html_path), open_browser=False, notebook=False)  # = generate & save
        with open(html_path, "r", encoding="utf-8") as f:
            html = f.read()
        st.components.v1.html(html, height=650, scrolling=True)
    else:
        st.info("å¯è¦–åŒ–ã«ã¯ networkx / pyvis ãŒå¿…è¦ã§ã™ã€‚è¡¨ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ã¿è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")


# ========= UIï¼šãƒˆãƒ¬ãƒ³ãƒ‰ =========
def _render_trend_block(df: pd.DataFrame):
    st.subheader("â‘¢ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆçµŒå¹´å¤‰åŒ–ï¼‰")

    # å¹´ãƒ¬ãƒ³ã‚¸
    if "ç™ºè¡Œå¹´" in df.columns:
        y = pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce")
        ymin, ymax = (int(y.min()), int(y.max())) if y.notna().any() else (1980, 2025)
    else:
        ymin, ymax = 1980, 2025

    c1, c2, c3 = st.columns([1.2, 1, 1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax, value=(ymin, ymax), key="kw_trend_year")
    with c2:
        topN = st.number_input("ä¸Šä½èªï¼ˆå¯è¦–åŒ–ï¼‰", min_value=5, max_value=50, value=15, step=5, key="kw_trend_topn")
    with c3:
        smooth = st.toggle("ç§»å‹•å¹³å‡ï¼ˆ3å¹´ï¼‰ã§å¹³æ»‘åŒ–", value=False, key="kw_trend_smooth")

    c4, c5 = st.columns([1, 1])
    with c4:
        tg_raw = {t for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
        tg_all = sorted(tg_raw)
        tg_sel = st.multiselect("å¯¾è±¡ç‰©ã§çµã‚Šè¾¼ã¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", tg_all, default=[], key="kw_trend_tg")
    with c5:
        tp_raw = {t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
        tp_all = sorted(tp_raw)
        tp_sel = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§çµã‚Šè¾¼ã¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", tp_all, default=[], key="kw_trend_tp")

    ts = _trend_timeseries_cached(df, y_from, y_to, tg_sel, tp_sel, int(topN))
    if ts.empty:
        st.info("æ¡ä»¶ã«åˆã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    if smooth:
        # 3å¹´ç§»å‹•å¹³å‡
        ts = ts.sort_values(["keyword", "year"]).copy()
        ts["freq_smooth"] = ts.groupby("keyword")["freq"].transform(lambda s: s.rolling(3, 1).mean())
        y_col = "freq_smooth"
    else:
        y_col = "freq"

    if HAS_PX:
        fig = px.line(ts, x="year", y=y_col, color="keyword",
                      labels={"year": "å¹´", y_col: "å‡ºç¾æ•°ï¼ˆå¹³æ»‘åŒ–ï¼‰" if smooth else "å‡ºç¾æ•°"})
        fig.update_layout(legend_title_text="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", height=520, margin=dict(l=10, r=10, t=30, b=10))
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.line_chart(ts.pivot_table(index="year", columns="keyword", values=y_col, aggfunc="sum").sort_index())

    st.caption("â€» ãƒˆãƒ¬ãƒ³ãƒ‰ã¯æœ¬æ–‡ãƒ»è¦æ—¨ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã®å˜ç´”é »åº¦ã‚’å¹´åˆ¥é›†è¨ˆã—ã¦ã„ã¾ã™ã€‚")


# ========= ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–¢æ•° =========
def render_keyword_tab(df: pd.DataFrame) -> None:
    st.markdown("## ğŸ§  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ")
    sub1, sub2, sub3 = st.tabs(["â‘  é »å‡º", "â‘¡ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰"])
    with sub1:
        _render_freq_block(df)
    with sub2:
        _render_cooccur_block(df)
    with sub3:
        _render_trend_block(df)