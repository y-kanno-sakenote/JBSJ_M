# modules/analysis/keywords.py
# -*- coding: utf-8 -*-
"""
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã‚¿ãƒ–ï¼ˆè»½é‡åŒ–å¯¾å¿œç‰ˆï¼‰
- â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æï¼šå¹´ãƒ»å¯¾è±¡ç‰©ãƒ»ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§çµã‚Šè¾¼ã¿ã€ä¸Šä½ã‚’æ£’ã‚°ãƒ©ãƒ•ã¨ï¼ˆä»»æ„ï¼‰ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
- â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼šåŒä¸€è«–æ–‡å†…å…±èµ· â†’ ã¾ãšè¡¨ã§ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€æç”»ã¯ã€Œãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸæ™‚ã ã‘ã€PyVisè¡¨ç¤º
- â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼šå¹´æ¯ã®å‡ºç¾é »åº¦ã‚’å¯è¦–åŒ–ï¼ˆPlotlyãŒç„¡ã‘ã‚Œã°stç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

â€» æ—¢å­˜ä»•æ§˜ã¯ç¶­æŒã€‚å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®â€œè‡ªå‹•æç”»â€ã®ã¿ã€Expander+ãƒœã‚¿ãƒ³ã«å¤‰æ›´ã€‚
"""

from __future__ import annotations
import re
import itertools
from typing import List, Tuple, Dict, Iterable

import pandas as pd
import streamlit as st

# ---- Optional deps ----
try:
    import plotly.express as px  # type: ignore
    HAS_PX = True
except Exception:
    HAS_PX = False

try:
    import networkx as nx  # type: ignore
    HAS_NX = True
except Exception:
    HAS_NX = False

try:
    from pyvis.network import Network  # type: ignore
    HAS_PYVIS = True
except Exception:
    HAS_PYVIS = False

# ---- æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆä»»æ„ï¼‰----
try:
    from modules.common.cache_utils import cache_csv_path, load_csv_if_exists, save_csv
    HAS_DISK_CACHE = True
except Exception:
    HAS_DISK_CACHE = False


# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
_SPLIT_MULTI_RE = re.compile(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+")

DEFAULT_KEYWORD_COLS = [
    "llm_keywords", "primary_keywords", "secondary_keywords", "featured_keywords",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰4","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰5",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰6","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰7","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰8","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰9","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰10",
]

def split_multi(s) -> List[str]:
    if not s: return []
    return [w.strip() for w in _SPLIT_MULTI_RE.split(str(s)) if w.strip()]

def norm_key(s: str) -> str:
    s = str(s or "").replace("\u00A0"," ")
    s = re.sub(r"\s+", " ", s).strip()
    return s.lower()

def col_contains_any(df_col: pd.Series, needles: List[str]) -> pd.Series:
    if not needles:
        return pd.Series([True]*len(df_col), index=df_col.index)
    lo = [norm_key(x) for x in needles]
    def _hit(v: str) -> bool:
        s = norm_key(v)
        return any(n in s for n in lo)
    return df_col.fillna("").astype(str).map(_hit)

@st.cache_data(ttl=600, show_spinner=False)
def _year_min_max(df: pd.DataFrame) -> tuple[int,int]:
    if "ç™ºè¡Œå¹´" not in df.columns: return (1980, 2025)
    y = pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce")
    if y.notna().any():
        return (int(y.min()), int(y.max()))
    return (1980, 2025)

def _apply_filters(df: pd.DataFrame,
                   y_from: int, y_to: int,
                   targets: List[str], types: List[str]) -> pd.DataFrame:
    use = df.copy()
    if "ç™ºè¡Œå¹´" in use.columns:
        y = pd.to_numeric(use["ç™ºè¡Œå¹´"], errors="coerce")
        use = use[(y >= y_from) & (y <= y_to) | y.isna()]
    if targets and "å¯¾è±¡ç‰©_top3" in use.columns:
        use = use[col_contains_any(use["å¯¾è±¡ç‰©_top3"], targets)]
    if types and "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in use.columns:
        use = use[col_contains_any(use["ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3"], types)]
    return use

def _available_keyword_cols(df: pd.DataFrame) -> List[str]:
    cols = [c for c in DEFAULT_KEYWORD_COLS if c in df.columns]
    # å¿µã®ãŸã‚ã€"keyword" ã‚’å«ã‚€åˆ—ã‚‚è¿½åŠ ï¼ˆé‡è¤‡æ’é™¤ï¼‰
    cols += [c for c in df.columns if ("keyword" in str(c).lower() and c not in cols)]
    return cols

def _iter_keywords_row(row: pd.Series, kw_cols: List[str]) -> Iterable[str]:
    for c in kw_cols:
        for w in split_multi(row.get(c, "")):
            yield w

# ========= â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ =========
@st.cache_data(ttl=600, show_spinner=False)
def _keyword_freq(df: pd.DataFrame, kw_cols: List[str]) -> pd.Series:
    bag: List[str] = []
    for _, r in df.iterrows():
        bag += list(_iter_keywords_row(r, kw_cols))
    if not bag:
        return pd.Series(dtype=int)
    s = pd.Series(bag, dtype="object")
    return s.value_counts().sort_values(ascending=False)

def _render_freq_block(df: pd.DataFrame) -> None:
    st.markdown("### â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
    kw_cols = _available_keyword_cols(df)
    if not kw_cols:
        st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    ymin, ymax = _year_min_max(df)
    c1,c2,c3 = st.columns([1,1,1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax, value=(ymin, ymax), key="kw_freq_year")
    with c2:
        tg_sel = st.text_input("å¯¾è±¡ç‰©ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä»»æ„ãƒ»ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", value="", key="kw_freq_tg")
        tg_needles = [w for w in _SPLIT_MULTI_RE.split(tg_sel) if w.strip()]
    with c3:
        tp_sel = st.text_input("ç ”ç©¶ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä»»æ„ãƒ»ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", value="", key="kw_freq_tp")
        tp_needles = [w for w in _SPLIT_MULTI_RE.split(tp_sel) if w.strip()]

    use = _apply_filters(df, y_from, y_to, tg_needles, tp_needles)
    freq = _keyword_freq(use, kw_cols)

    if freq.empty:
        st.info("æ¡ä»¶ã«åˆã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    top_n = st.number_input("è¡¨ç¤ºä»¶æ•°", min_value=10, max_value=200, value=30, step=10, key="kw_freq_topn")
    freq_top = freq.head(int(top_n))

# --- ã“ã“ã‹ã‚‰å·®ã—æ›¿ãˆï¼š_render_freq_block å†…ã®ãƒãƒ¼æç”»ãƒ‘ãƒ¼ãƒˆä¸¸ã”ã¨ ---
    def _freq_to_df(freq_top):
        """Series / DataFrame ã„ãšã‚Œã®å½¢ã§ã‚‚ 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰','ä»¶æ•°' ã‚’æƒãˆã‚‹å®‰å…¨å¤‰æ›"""
        import pandas as pd
        if isinstance(freq_top, pd.Series):
            df = freq_top.rename("ä»¶æ•°").reset_index().rename(columns={"index": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"})
        else:
            df = freq_top.reset_index()
            # ä»¶æ•°åˆ—ã®æ¨å®š
            if "ä»¶æ•°" not in df.columns:
                # index ä»¥å¤–ã®æœ€åˆã®åˆ—ã‚’ä»¶æ•°æ‰±ã„ï¼ˆv.value_counts() ç”±æ¥ã‚’æƒ³å®šï¼‰
                cand = [c for c in df.columns if c != "index"]
                if cand:
                    df = df.rename(columns={cand[0]: "ä»¶æ•°"})
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã®çµ±ä¸€
            if "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" not in df.columns and "index" in df.columns:
                df = df.rename(columns={"index": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"})
        if "ä»¶æ•°" in df.columns:
            df["ä»¶æ•°"] = pd.to_numeric(df["ä»¶æ•°"], errors="coerce").fillna(0).astype(int)
        return df

    # ã“ã“ã§ freq_top ã‚’ DataFrame åŒ–ã—ã¦ã‹ã‚‰æç”»
    freq_df = _freq_to_df(freq_top)

    if freq_df.empty:
        st.info("è¡¨ç¤ºã§ãã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ¡ä»¶ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
    else:
        try:
            import plotly.express as px
            fig = px.bar(
                freq_df,
                x="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                y="ä»¶æ•°",
                text_auto=True,
                title="é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸Šä½ï¼‰",
            )
            fig.update_layout(margin=dict(l=10, r=10, t=40, b=10), height=420)
            st.plotly_chart(fig, use_container_width=True)
        except Exception:
            # Plotly ãŒç„¡ã„/ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            st.bar_chart(freq_df.set_index("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")["ä»¶æ•°"])

    with st.expander("ğŸ§© ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰", expanded=False):
        try:
            from wordcloud import WordCloud  # type: ignore
            words = {k:int(v) for k,v in freq_top.to_dict().items()}
            if st.button("â˜ï¸ ç”Ÿæˆã™ã‚‹", key="kw_wc_btn"):
                wc = WordCloud(width=900, height=400, background_color="white",
                               font_path=None).generate_from_frequencies(words)
                st.image(wc.to_array(), use_column_width=True)
        except Exception:
            st.caption("wordcloud ãŒæœªå°å…¥ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")


# ========= â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ =========
@st.cache_data(ttl=600, show_spinner=False)
def _build_keyword_cooccur_edges(df: pd.DataFrame, kw_cols: List[str],
                                 min_edge: int) -> pd.DataFrame:
    """
    åŒä¸€è«–æ–‡å†…ã§ä¸€ç·’ã«å‡ºãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åŒå£«ã‚’1ã‚«ã‚¦ãƒ³ãƒˆã€‚
    æˆ»ã‚Šå€¤: ['src','dst','weight']
    """
    rows: List[Tuple[str,str]] = []
    for _, r in df.iterrows():
        terms = list(dict.fromkeys([w for w in _iter_keywords_row(r, kw_cols)]))
        if len(terms) < 2:
            continue
        for a, b in itertools.combinations(sorted(terms), 2):
            rows.append((a,b))
    if not rows:
        return pd.DataFrame(columns=["src","dst","weight"])
    edges = (pd.DataFrame(rows, columns=["src","dst"])
             .value_counts()
             .reset_index(name="weight"))
    edges = edges[edges["weight"] >= int(min_edge)].sort_values("weight", ascending=False).reset_index(drop=True)
    return edges

def _draw_pyvis_from_edges(edges: pd.DataFrame, height_px: int = 680) -> None:
    """PyVisã®HTMLç”Ÿæˆã‚’ä½¿ã£ã¦åŸ‹ã‚è¾¼ã‚€ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•èµ·å‹•ã‚’å›é¿ï¼‰"""
    if not (HAS_PYVIS and HAS_NX):
        st.info("networkx / pyvis ãŒæœªå°å…¥ã®ãŸã‚ã€æç”»ã§ãã¾ã›ã‚“ã€‚")
        return
    if edges.empty:
        st.warning("æç”»å¯¾è±¡ã®ã‚¨ãƒƒã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    G = nx.Graph()
    for _, r in edges.iterrows():
        s, t, w = r["src"], r["dst"], int(r["weight"])
        if G.has_edge(s,t):
            G[s][t]["weight"] += w
        else:
            G.add_edge(s,t,weight=w)

    net = Network(height=f"{height_px}px", width="100%", bgcolor="#ffffff", font_color="#222")
    net.barnes_hut(gravity=-30000, central_gravity=0.25, spring_length=120, spring_strength=0.02)
    net.from_nx(G)
    html = net.generate_html(notebook=False)  # â† ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼ˆopen_browserã—ãªã„ï¼‰
    st.components.v1.html(html, height=height_px, scrolling=True)

def _render_cooccur_block(df: pd.DataFrame) -> None:
    st.markdown("### â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆè»½é‡è¡¨ç¤ºï¼‰")
    kw_cols = _available_keyword_cols(df)
    if not kw_cols:
        st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    ymin, ymax = _year_min_max(df)
    c1, c2, c3, c4 = st.columns([1,1,1,1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax, value=(ymin, ymax), key="kw_net_year")
    with c2:
        min_edge = st.number_input("ã‚¨ãƒƒã‚¸æœ€å°å›æ•° (wâ‰¥)", min_value=1, max_value=50, value=3, step=1, key="kw_net_minw")
    with c3:
        tg_sel = st.text_input("å¯¾è±¡ç‰©ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä»»æ„ï¼‰", value="", key="kw_net_tg")
        tg_needles = [w for w in _SPLIT_MULTI_RE.split(tg_sel) if w.strip()]
    with c4:
        tp_sel = st.text_input("ç ”ç©¶ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä»»æ„ï¼‰", value="", key="kw_net_tp")
        tp_needles = [w for w in _SPLIT_MULTI_RE.split(tp_sel) if w.strip()]

    # è»½é‡ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
    use = _apply_filters(df, y_from, y_to, tg_needles, tp_needles)

    # ---- æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ ----
    cache_key = f"kwco|{y_from}-{y_to}|min{min_edge}|tg{','.join(tg_needles)}|tp{','.join(tp_needles)}"

    # ã‚¨ãƒƒã‚¸æ§‹ç¯‰ï¼ˆé‡ã„ã®ã§ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆï¼‰
    edges = None
    if HAS_DISK_CACHE:
        p = cache_csv_path("kw_co_edges", cache_key)
        cached = load_csv_if_exists(p)
        if cached is not None:
            edges = cached

    if edges is None:
        edges = _build_keyword_cooccur_edges(use, kw_cols, int(min_edge))
        if HAS_DISK_CACHE:
            save_csv(edges, cache_csv_path("kw_co_edges", cache_key))

    st.caption(f"ã‚¨ãƒƒã‚¸æ•°: {len(edges)}ï¼ˆå…ˆé ­200ä»¶ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰")
    st.dataframe(edges.head(200), use_container_width=True, hide_index=True)

    # ==== ã“ã“ãŒå¤‰æ›´ç‚¹ï¼šæç”»ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œæ™‚ã®ã¿ ====
    with st.expander("ğŸ•¸ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æç”»ï¼ˆä»»æ„ãƒ»é‡ã„å‡¦ç†ï¼‰", expanded=False):
        st.caption("ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨PyVisã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æç”»ã—ã¾ã™ã€‚")
        if st.button("ğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æç”»ã™ã‚‹", key="kw_net_draw"):
            _draw_pyvis_from_edges(edges, height_px=680)


# ========= â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆçµŒå¹´ï¼‰ =========
@st.cache_data(ttl=600, show_spinner=False)
def _yearly_keyword_counts(df: pd.DataFrame, kw_cols: List[str]) -> pd.DataFrame:
    """å¹´Ã—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä»¶æ•°ï¼ˆè«–æ–‡å˜ä½ã§é‡è¤‡æ’é™¤ï¼‰"""
    if "ç™ºè¡Œå¹´" not in df.columns:
        return pd.DataFrame(columns=["ç™ºè¡Œå¹´","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰","count"])
    rows = []
    for _, r in df.iterrows():
        y = pd.to_numeric(r.get("ç™ºè¡Œå¹´"), errors="coerce")
        if pd.isna(y): 
            continue
        items = list(dict.fromkeys([w for w in _iter_keywords_row(r, kw_cols)]))
        for it in items:
            rows.append((int(y), it))
    if not rows:
        return pd.DataFrame(columns=["ç™ºè¡Œå¹´","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰","count"])
    c = (pd.DataFrame(rows, columns=["ç™ºè¡Œå¹´","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"])
         .value_counts()
         .reset_index(name="count"))
    return c.sort_values(["ç™ºè¡Œå¹´","count"], ascending=[True, False]).reset_index(drop=True)

def _render_trend_block(df: pd.DataFrame) -> None:
    st.markdown("### â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆçµŒå¹´å¤‰åŒ–ï¼‰")
    kw_cols = _available_keyword_cols(df)
    if not kw_cols:
        st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    ymin, ymax = _year_min_max(df)
    c1,c2,c3 = st.columns([1,1,1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax, value=(ymin, ymax), key="kw_trend_year")
    with c2:
        tg_sel = st.text_input("å¯¾è±¡ç‰©ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä»»æ„ï¼‰", value="", key="kw_trend_tg")
        tg_needles = [w for w in _SPLIT_MULTI_RE.split(tg_sel) if w.strip()]
    with c3:
        tp_sel = st.text_input("ç ”ç©¶ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä»»æ„ï¼‰", value="", key="kw_trend_tp")
        tp_needles = [w for w in _SPLIT_MULTI_RE.split(tp_sel) if w.strip()]

    use = _apply_filters(df, y_from, y_to, tg_needles, tp_needles)
    yearly = _yearly_keyword_counts(use, kw_cols)
    if yearly.empty:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    all_terms = yearly["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"].value_counts().head(500).index.tolist()
    sel = st.multiselect("è¡¨ç¤ºã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", all_terms[:1000], default=all_terms[: min(8, len(all_terms))], key="kw_trend_sel")

    piv = (yearly[yearly["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"].isin(sel)]
           .pivot_table(index="ç™ºè¡Œå¹´", columns="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", values="count", aggfunc="sum")
           .fillna(0).sort_index())

    if piv.empty:
        st.info("é¸æŠèªã®ç³»åˆ—ãŒç©ºã§ã™ã€‚")
        return

    if HAS_PX:
        fig = px.line(piv.reset_index().melt(id_vars="ç™ºè¡Œå¹´", var_name="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", value_name="ä»¶æ•°"),
                      x="ç™ºè¡Œå¹´", y="ä»¶æ•°", color="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", markers=True)
        fig.update_layout(height=520, margin=dict(l=10,r=10,t=30,b=10))
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.line_chart(piv)


# ========= ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼šã‚¿ãƒ–æœ¬ä½“ =========
def render_keyword_tab(df: pd.DataFrame) -> None:
    st.markdown("## ğŸ§  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ")
    tab1, tab2, tab3 = st.tabs([
        "â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
        "â‘¡ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰",
    ])
    with tab1:
        _render_freq_block(df)
    with tab2:
        _render_cooccur_block(df)   # â† æç”»ã¯ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã®ã¿
    with tab3:
        _render_trend_block(df)