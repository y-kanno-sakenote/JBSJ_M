# modules/analysis/keywords.py
# -*- coding: utf-8 -*-
"""
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã‚¿ãƒ–ï¼ˆå®Œæˆç‰ˆãƒ»å®‰å…¨ãªé…å»¶å®Ÿè¡Œï¼†ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰

æ©Ÿèƒ½ï¼ˆå¾“æ¥ã©ãŠã‚Šï¼‰:
â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
   - å¹´ãƒ»å¯¾è±¡ç‰©ãƒ»ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§çµã‚Šè¾¼ã¿
   - å‡ºç¾å›æ•°ä¸Šä½ã‚’ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆè¡¨ç¤º
   - WordCloudï¼ˆwordcloud ãŒã‚ã‚Œã°ï¼‰ã‚’ä»»æ„è¡¨ç¤º

â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆé‡ã„ã®ã§é…å»¶æç”»ï¼‰
   - åŒä¸€è«–æ–‡å†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…±èµ·ã‚’ networkx + pyvis ã§å¯è¦–åŒ–
   - ã€Œãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æç”»ã€ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã®ã¿ç”Ÿæˆ
   - ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆmodules/common/cache_utils.pyï¼‰å¯¾å¿œ

â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆçµŒå¹´å¤‰åŒ–ï¼‰
   - å¹´ã”ã¨ã«å‡ºç¾é »åº¦ã‚’é›†è¨ˆã—ã€TopNèªã‚’æŠ˜ã‚Œç·šã§å¯è¦–åŒ–ï¼ˆPlotlyãŒãªã‘ã‚Œã°st.line_chartï¼‰

æ³¨æ„ï¼š
- importæ™‚ã«é‡ã„å‡¦ç†ã‚’ä¸€åˆ‡èµ°ã‚‰ã›ã¾ã›ã‚“ï¼ˆé–¢æ•°å†…ã®ã¿ã§å®Ÿè¡Œï¼‰
- ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆkeyã¯ "kw_*" æ¥é ­ã§ä»–ã‚¿ãƒ–ã¨è¡çªã—ãªã„ã‚ˆã†ã«ã—ã¦ã„ã¾ã™
"""

from __future__ import annotations
import re
from typing import List, Tuple, Dict

import pandas as pd
import streamlit as st

from pathlib import Path

def _get_japanese_font_path() -> str | None:
    """æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’è¿”ã™ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåŒæ¢±ã‚’æœ€å„ªå…ˆã€‚"""
    candidates = [
        "fonts/IPAexGothic.ttf",                            # â† åŒæ¢±æ¨å¥¨
        "/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf",
        "/usr/share/fonts/opentype/ipafont-mincho/ipam.ttf",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc",      # mac
    ]
    for p in candidates:
        if Path(p).exists():
            return p
    return None

# ==== Optional depsï¼ˆç„¡ãã¦ã‚‚å‹•ãï¼‰ ====
try:
    import plotly.express as px  # type: ignore
    HAS_PX = True
except Exception:
    HAS_PX = False

try:
    from wordcloud import WordCloud  # type: ignore
    HAS_WC = True
except Exception:
    HAS_WC = False

try:
    import networkx as nx  # type: ignore
    HAS_NX = True
except Exception:
    HAS_NX = False

try:
    from pyvis.network import Network  # type: ignore
    HAS_PYVIS = True
except Exception:
    HAS_PYVIS = False

# æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥IOï¼ˆã‚ã‚Œã°ä½¿ã†ï¼‰
try:
    from modules.common.cache_utils import cache_csv_path, load_csv_if_exists, save_csv
    HAS_DISK_CACHE = True
except Exception:
    HAS_DISK_CACHE = False


# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
_SPLIT_MULTI_RE = re.compile(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+")

KEY_COLS = [
    "llm_keywords","primary_keywords","secondary_keywords","featured_keywords",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰4","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰5",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰6","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰7","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰8","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰9","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰10",
]

def norm_key(s: str) -> str:
    s = str(s or "").replace("\u00A0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s.lower()

def split_multi(s) -> List[str]:
    if not s:
        return []
    return [w.strip() for w in _SPLIT_MULTI_RE.split(str(s)) if w.strip()]

def col_contains_any(df_col: pd.Series, needles: List[str]) -> pd.Series:
    if not needles:
        return pd.Series([True] * len(df_col), index=df_col.index)
    lo_needles = [norm_key(n) for n in needles]
    def _hit(v: str) -> bool:
        s = norm_key(v)
        return any(n in s for n in lo_needles)
    return df_col.fillna("").astype(str).map(_hit)

@st.cache_data(ttl=600, show_spinner=False)
def year_min_max(df: pd.DataFrame) -> Tuple[int, int]:
    if "ç™ºè¡Œå¹´" not in df.columns:
        return (1980, 2025)
    y = pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce")
    if y.notna().any():
        return (int(y.min()), int(y.max()))
    return (1980, 2025)

def _apply_filters(df: pd.DataFrame,
                   y_from: int, y_to: int,
                   targets: List[str], types: List[str]) -> pd.DataFrame:
    use = df.copy()
    if "ç™ºè¡Œå¹´" in use.columns:
        y = pd.to_numeric(use["ç™ºè¡Œå¹´"], errors="coerce")
        use = use[(y >= y_from) & (y <= y_to) | y.isna()]
    if targets and "å¯¾è±¡ç‰©_top3" in use.columns:
        use = use[col_contains_any(use["å¯¾è±¡ç‰©_top3"], targets)]
    if types and "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in use.columns:
        use = use[col_contains_any(use["ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3"], types)]
    return use

def _extract_keywords_from_row(row: pd.Series) -> List[str]:
    words: List[str] = []
    for c in KEY_COLS:
        if c in row and pd.notna(row[c]):
            words += split_multi(row[c])
    return [w for w in words if w]

@st.cache_data(ttl=600, show_spinner=False)
def collect_keywords(df: pd.DataFrame) -> pd.Series:
    """å…¨è¡Œã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã‚’æŠ½å‡ºã—ã¦1æœ¬ã®Seriesã«"""
    bags: List[str] = []
    for _, r in df.iterrows():
        bags += _extract_keywords_from_row(r)
    return pd.Series(bags, dtype="object")

@st.cache_data(ttl=600, show_spinner=False)
def keyword_freq(df: pd.DataFrame) -> pd.Series:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é »åº¦ï¼ˆé™é †ï¼‰"""
    s = collect_keywords(df)
    if s.empty:
        return pd.Series(dtype=int)
    return s.value_counts().sort_values(ascending=False)

@st.cache_data(ttl=600, show_spinner=False)
def yearly_keyword_counts(df: pd.DataFrame) -> pd.DataFrame:
    """å¹´Ã—èªã®ä»¶æ•°ï¼ˆè«–æ–‡ã”ã¨é‡è¤‡é™¤å»ï¼‰"""
    if "ç™ºè¡Œå¹´" not in df.columns:
        return pd.DataFrame(columns=["ç™ºè¡Œå¹´", "keyword", "count"])
    rows = []
    for _, r in df.iterrows():
        y = pd.to_numeric(r.get("ç™ºè¡Œå¹´"), errors="coerce")
        if pd.isna(y): 
            continue
        kws = list(dict.fromkeys(_extract_keywords_from_row(r)))
        for k in kws:
            rows.append((int(y), k))
    if not rows:
        return pd.DataFrame(columns=["ç™ºè¡Œå¹´", "keyword", "count"])
    c = pd.DataFrame(rows, columns=["ç™ºè¡Œå¹´","keyword"]).value_counts().reset_index(name="count")
    return c.sort_values(["ç™ºè¡Œå¹´","count"], ascending=[True, False]).reset_index(drop=True)

# ====== å…±èµ·ã‚¨ãƒƒã‚¸ï¼ˆé‡ã„ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰ ======
@st.cache_data(ttl=600, show_spinner=False)
def build_keyword_cooccur_edges(df: pd.DataFrame, min_edge: int) -> pd.DataFrame:
    """
    åŒä¸€è«–æ–‡å†…ã§å…±èµ·ã™ã‚‹èªã®ãƒšã‚¢ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    æˆ»ã‚Šå€¤: ['src','dst','weight']
    """
    rows = []
    for _, r in df.iterrows():
        kws = sorted(set(_extract_keywords_from_row(r)))
        # å…¨çµ„åˆã›
        for i in range(len(kws)):
            for j in range(i+1, len(kws)):
                rows.append((kws[i], kws[j]))
    if not rows:
        return pd.DataFrame(columns=["src","dst","weight"])
    edges = pd.DataFrame(rows, columns=["src","dst"]).value_counts().reset_index(name="weight")
    edges = edges[edges["weight"] >= int(min_edge)].sort_values("weight", ascending=False).reset_index(drop=True)
    return edges

def _freq_to_df(freq: pd.Series, topn: int) -> pd.DataFrame:
    if freq.empty:
        return pd.DataFrame(columns=["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰","ä»¶æ•°"])
    df = freq.head(int(topn)).reset_index()
    df.columns = ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰","ä»¶æ•°"]
    return df

def _draw_pyvis_from_edges(edges: pd.DataFrame, height_px: int = 650) -> None:
    if not (HAS_NX and HAS_PYVIS):
        st.info("networkx / pyvis ãŒæœªå°å…¥ã®ãŸã‚ã€è¡¨ã®ã¿è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")
        return
    if edges.empty:
        st.warning("å¯¾è±¡æ¡ä»¶ã§ã‚¨ãƒƒã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # æ–‡å­—åˆ—IDã«çµ±ä¸€
    G = nx.Graph()
    for _, r in edges.iterrows():
        s = str(r["src"]); t = str(r["dst"]); w = int(r["weight"])
        if G.has_edge(s, t):
            G[s][t]["weight"] += w
        else:
            G.add_edge(s, t, weight=w)

    net = Network(height=f"{height_px}px", width="100%", bgcolor="#ffffff", font_color="#222")
    net.barnes_hut(gravity=-30000, central_gravity=0.25, spring_length=120, spring_strength=0.02)
    net.from_nx(G)
    html = net.generate_html(notebook=False)  # â† ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•ã‚ªãƒ¼ãƒ—ãƒ³å›é¿
    st.components.v1.html(html, height=height_px, scrolling=True)


# ========= â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ =========
def _render_freq_block(df: pd.DataFrame) -> None:
    st.markdown("### â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")

    ymin, ymax = year_min_max(df)
    c1, c2, c3, c4 = st.columns([1,1,1,1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax,
                                 value=(ymin, ymax), key="kw_freq_year")
    with c2:
        tg_txt = st.text_input("å¯¾è±¡ç‰©ã§çµã‚Šè¾¼ã¿ï¼ˆç©ºç™½åŒºåˆ‡ã‚Šãƒ»éƒ¨åˆ†ä¸€è‡´ï¼‰", value="", key="kw_freq_tg")
        tg_needles = [w for w in _SPLIT_MULTI_RE.split(tg_txt) if w.strip()]
    with c3:
        tp_txt = st.text_input("ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§çµã‚Šè¾¼ã¿ï¼ˆç©ºç™½åŒºåˆ‡ã‚Šãƒ»éƒ¨åˆ†ä¸€è‡´ï¼‰", value="", key="kw_freq_tp")
        tp_needles = [w for w in _SPLIT_MULTI_RE.split(tp_txt) if w.strip()]
    with c4:
        topn = st.number_input("è¡¨ç¤ºä»¶æ•°", min_value=5, max_value=100, value=30, step=5, key="kw_freq_topn")

    use = _apply_filters(df, y_from, y_to, tg_needles, tp_needles)
    freq = keyword_freq(use)
    freq_df = _freq_to_df(freq, int(topn))

    if freq_df.empty:
        st.info("æ¡ä»¶ã«åˆã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
    if HAS_PX:
        fig = pd.DataFrame(freq_df)  # æ˜ç¤º
        fig = px.bar(fig, x="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", y="ä»¶æ•°", text_auto=True, title="é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸Šä½ï¼‰")
        fig.update_layout(margin=dict(l=10,r=10,t=40,b=10), height=420)
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.bar_chart(freq_df.set_index("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")["ä»¶æ•°"])

    # WordCloudï¼ˆä»»æ„ï¼‰
    with st.expander("â˜ WordCloudï¼ˆä»»æ„ï¼‰", expanded=False):
        if HAS_WC:
            if st.button("ç”Ÿæˆã™ã‚‹", key="kw_wc_btn"):
                # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è§£æ±º
                font_path = _get_japanese_font_path()
                wc_kwargs = dict(
                    width=900, height=450, background_color="white",
                    prefer_horizontal=1.0, collocations=False
                )
                if font_path:
                    wc_kwargs["font_path"] = font_path
                else:
                    st.warning("æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`fonts/IPAexGothic.ttf` ã‚’ç½®ãã¨æ–‡å­—åŒ–ã‘ã—ã¾ã›ã‚“ã€‚")

                # freq_df â†’ dict ã«æ˜ç¤ºå¤‰æ›ï¼ˆå‹ã®æºã‚Œå¯¾ç­–ï¼‰
                freq_dict = {str(row["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]): int(row["ä»¶æ•°"]) for _, row in freq_df.iterrows()}

                # ç”Ÿæˆ
                wc = WordCloud(**wc_kwargs).generate_from_frequencies(freq_dict)

                # PILç”»åƒã¨ã—ã¦å®‰å…¨ã«è¡¨ç¤ºï¼ˆmatplotlibä¸ä½¿ç”¨ï¼‰
                import io
                buf = io.BytesIO()
                img = wc.to_image()
                img.save(buf, format="PNG")
                buf.seek(0)
                st.image(buf, use_container_width=True)
        else:
            st.caption("â€» wordcloud ãŒæœªå°å…¥ã®ãŸã‚éè¡¨ç¤ºã§ã™ã€‚")
            
# ========= â‘¡ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆé…å»¶æç”»ï¼‰ =========
def _render_cooccur_block(df: pd.DataFrame) -> None:
    st.markdown("### â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")

    ymin, ymax = year_min_max(df)
    c1, c2, c3, c4 = st.columns([1,1,1,1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax,
                                 value=(ymin, ymax), key="kw_co_year")
    with c2:
        min_edge = st.number_input("ã‚¨ãƒƒã‚¸æœ€å°å›æ•° (wâ‰¥)", min_value=1, max_value=50, value=3, step=1, key="kw_co_minw")
    with c3:
        topN = st.number_input("ãƒãƒ¼ãƒ‰ä¸Šé™ï¼ˆå‡ºç¾ä¸Šä½ï¼‰", min_value=30, max_value=300, value=120, step=10, key="kw_co_topn")
    with c4:
        st.caption("é‡ã„ã®ã§ä¸‹ã®ãƒœã‚¿ãƒ³ã§æ˜ç¤ºçš„ã«æç”»ã—ã¾ã™ã€‚")

    c5, c6 = st.columns([1,1])
    with c5:
        tg_txt = st.text_input("å¯¾è±¡ç‰©ã§çµã‚Šè¾¼ã¿ï¼ˆç©ºç™½åŒºåˆ‡ã‚Šãƒ»éƒ¨åˆ†ä¸€è‡´ï¼‰", value="", key="kw_co_tg")
        tg_needles = [w for w in _SPLIT_MULTI_RE.split(tg_txt) if w.strip()]
    with c6:
        tp_txt = st.text_input("ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§çµã‚Šè¾¼ã¿ï¼ˆç©ºç™½åŒºåˆ‡ã‚Šãƒ»éƒ¨åˆ†ä¸€è‡´ï¼‰", value="", key="kw_co_tp")
        tp_needles = [w for w in _SPLIT_MULTI_RE.split(tp_txt) if w.strip()]

    use = _apply_filters(df, y_from, y_to, tg_needles, tp_needles)

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼
    cache_key = f"kwco|{y_from}-{y_to}|min{min_edge}|top{topN}|tg{','.join(tg_needles)}|tp{','.join(tp_needles)}"

    # 1) ã‚¨ãƒƒã‚¸æ§‹ç¯‰ï¼ˆé‡ã„ã®ã§æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
    edges = None
    if HAS_DISK_CACHE:
        path_edges = cache_csv_path("kw_co_edges", cache_key)
        cached = load_csv_if_exists(path_edges)
        if cached is not None:
            edges = cached

    if edges is None:
        edges = build_keyword_cooccur_edges(use, int(min_edge))
        # ä¸Šä½ãƒãƒ¼ãƒ‰ã ã‘ã«åˆ¶é™
        if not edges.empty and int(topN) > 0:
            deg = pd.concat([edges.groupby("src")["weight"].sum(),
                             edges.groupby("dst")["weight"].sum()], axis=1).fillna(0).sum(axis=1)
            keep_nodes = set(deg.sort_values(ascending=False).head(int(topN)).index.tolist())
            edges = edges[edges["src"].isin(keep_nodes) & edges["dst"].isin(keep_nodes)].reset_index(drop=True)
        if HAS_DISK_CACHE:
            save_csv(edges, path_edges)

    st.caption(f"ã‚¨ãƒƒã‚¸æ•°: {len(edges)}")
    st.dataframe(edges.head(200), use_container_width=True, hide_index=True)

    with st.expander("ğŸ•¸ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æç”»ï¼ˆPyVis / ä»»æ„ä¾å­˜ï¼‰", expanded=False):
        if HAS_PYVIS and HAS_NX:
            if st.button("ğŸŒ æç”»ã™ã‚‹", key="kw_co_draw"):
                _draw_pyvis_from_edges(edges, height_px=680)
        else:
            st.info("networkx / pyvis ãŒæœªå°å…¥ã®ãŸã‚ã€è¡¨ã®ã¿è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚")


# ========= â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆçµŒå¹´å¤‰åŒ–ï¼‰ =========
def _render_trend_block(df: pd.DataFrame) -> None:
    st.markdown("### â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆçµŒå¹´å¤‰åŒ–ï¼‰")

    ymin, ymax = year_min_max(df)
    c1, c2, c3 = st.columns([1,1,1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax,
                                 value=(ymin, ymax), key="kw_trend_year")
    with c2:
        topn = st.number_input("è¡¨ç¤ºã™ã‚‹èªæ•°ï¼ˆTopNï¼‰", min_value=5, max_value=50, value=15, step=5, key="kw_trend_topn")
    with c3:
        ma = st.number_input("ç§»å‹•å¹³å‡ï¼ˆå¹´ï¼‰", min_value=1, max_value=7, value=1, step=1, key="kw_trend_ma")

    use = _apply_filters(df, y_from, y_to, [], [])
    yearly = yearly_keyword_counts(use)
    if yearly.empty:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # æœ€æ–°å¹´ä»˜è¿‘ã®TopNèªã‚’é¸ã¶ï¼ˆå…¨ä½“ä¸Šä½ã ã¨å‡¡ä¾‹ãŒå¤šã™ãã‚‹ãŸã‚ï¼‰
    latest_year = yearly["ç™ºè¡Œå¹´"].max()
    latest_top = (yearly[yearly["ç™ºè¡Œå¹´"] == latest_year]
                  .sort_values("count", ascending=False)["keyword"]
                  .head(int(topn)).tolist())
    piv = (yearly[yearly["keyword"].isin(latest_top)]
           .pivot_table(index="ç™ºè¡Œå¹´", columns="keyword", values="count", aggfunc="sum")
           .fillna(0).sort_index())

    if int(ma) > 1:
        piv = piv.rolling(window=int(ma), min_periods=1).mean()

    if HAS_PX:
        fig = px.line(piv.reset_index().melt(id_vars="ç™ºè¡Œå¹´", var_name="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", value_name="ä»¶æ•°"),
                      x="ç™ºè¡Œå¹´", y="ä»¶æ•°", color="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", markers=True)
        fig.update_layout(height=520, margin=dict(l=10,r=10,t=30,b=10))
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.line_chart(piv)


# ========= ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼šã‚¿ãƒ–æœ¬ä½“ =========
def render_keyword_tab(df: pd.DataFrame) -> None:
    st.markdown("## ğŸ§  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ")

    tab1, tab2, tab3 = st.tabs([
        "â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
        "â‘¡ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ",
    ])

    with tab1:
        _render_freq_block(df)

    with tab2:
        _render_cooccur_block(df)   # â† é…å»¶æç”»ï¼ˆãƒœã‚¿ãƒ³å¼ï¼‰

    with tab3:
        _render_trend_block(df)