# modules/analysis/keywords.py
# -*- coding: utf-8 -*-
"""
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æï¼ˆé«˜é€Ÿç‰ˆãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ï¼‰
â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¡¨ / ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆ / ä»»æ„: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ï¼‰
â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆä»»æ„: networkx + pyvisï¼‰
â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆå¹´åˆ¥å‡ºç¾æ¨ç§»ï¼‰

ä¾å­˜ã¯ã™ã¹ã¦ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ã«ã—ã€æœªå°å…¥ã§ã‚‚æ©Ÿèƒ½ãŒè½ã¡ãªã„ã‚ˆã†ã«å®Ÿè£…ã€‚
"""

from __future__ import annotations
import re
import itertools
from collections import Counter, defaultdict
from typing import Iterable

import pandas as pd
import streamlit as st

# ---- Optional deps ----
try:
    import matplotlib.pyplot as plt
    HAS_MPL = True
except Exception:
    HAS_MPL = False

try:
    from wordcloud import WordCloud
    HAS_WC = True
except Exception:
    HAS_WC = False

try:
    import networkx as nx
    HAS_NX = True
except Exception:
    HAS_NX = False

try:
    from pyvis.network import Network
    HAS_PYVIS = True
except Exception:
    HAS_PYVIS = False


# =========================================================
# å‰å‡¦ç†ãƒ»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
# =========================================================

KW_COLS_DEFAULT = [
    "llm_keywords","primary_keywords","secondary_keywords","featured_keywords",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰4","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰5",
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰6","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰7","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰8","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰9","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰10",
]

_SPLIT_RE = re.compile(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+")

def _norm(s: str) -> str:
    s = str(s or "")
    s = s.replace("\u00A0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _norm_lc(s: str) -> str:
    return _norm(s).lower()

def _split_tokens(val: str) -> list[str]:
    if val is None:
        return []
    return [w.strip() for w in _SPLIT_RE.split(str(val)) if w.strip()]

def _collect_tokens_from_row(row: pd.Series, kw_cols: list[str]) -> list[str]:
    toks: list[str] = []
    for c in kw_cols:
        if c in row and pd.notna(row[c]):
            toks.extend(_split_tokens(row[c]))
    # ãƒã‚¤ã‚ºé™¤å» & å°æ–‡å­—åŒ–
    toks = [_norm_lc(t) for t in toks if t]
    # 1è«–æ–‡ã§åŒèªãŒé‡è¤‡ã—ã¦ã„ã¦ã‚‚OKï¼ˆé »åº¦ã¯ç©ã¿ä¸Šã’ã‚‹ï¼‰
    return toks

@st.cache_data(ttl=600, show_spinner=False)
def precompute_keywords_df(
    df: pd.DataFrame,
    kw_cols: tuple[str, ...]
) -> pd.DataFrame:
    """
    1åº¦ã ã‘å…¨è¡Œã‚’ãªã‚ã¦ã€å¾Œæ®µã®ãƒ•ã‚£ãƒ«ã‚¿ã«åŠ¹ãæƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã€‚
    æˆ»ã‚Šå€¤: DataFrame[ç™ºè¡Œå¹´_num, targets_lc, types_lc, tokens(list[str])]
    """
    use = df.copy()

    # å¹´
    if "ç™ºè¡Œå¹´" in use.columns:
        use["ç™ºè¡Œå¹´_num"] = pd.to_numeric(use["ç™ºè¡Œå¹´"], errors="coerce")
    else:
        use["ç™ºè¡Œå¹´_num"] = pd.Series([None] * len(use), dtype="float")

    # å¯¾è±¡ç‰©/ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆæ–‡å­—åˆ—åŒ–ãƒ»å°æ–‡å­—åŒ–ï¼‰
    tgt_col = "å¯¾è±¡ç‰©_top3" if "å¯¾è±¡ç‰©_top3" in use.columns else None
    typ_col = "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" if "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in use.columns else None

    use["targets_lc"] = use[tgt_col].astype(str).apply(_norm_lc) if tgt_col else ""
    use["types_lc"]   = use[typ_col].astype(str).apply(_norm_lc) if typ_col else ""

    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ¸ˆã¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    tokens_all = []
    for _, row in use.iterrows():
        toks = _collect_tokens_from_row(row, list(kw_cols))
        tokens_all.append(toks)
    use["tokens"] = tokens_all

    return use[["ç™ºè¡Œå¹´_num","targets_lc","types_lc","tokens"]].copy()

def _year_bounds(df: pd.DataFrame) -> tuple[int, int]:
    if "ç™ºè¡Œå¹´" in df.columns:
        y = pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce")
        if y.notna().any():
            return int(y.min()), int(y.max())
    return 1980, 2025

def _apply_filters(
    base: pd.DataFrame,
    y_from: int, y_to: int,
    targets_sel: list[str],
    types_sel: list[str]
) -> Iterable[list[str]]:
    """
    å‰è¨ˆç®—æ¸ˆã¿DF(base)ã«å¯¾ã—ã¦ã€å¹´ãƒ»å¯¾è±¡ç‰©ãƒ»ç ”ç©¶ã‚¿ã‚¤ãƒ—æ¡ä»¶ã§è¡Œã‚’çµã‚Šã€
    å„è¡Œã® tokens(list[str]) ã‚’é †ã«è¿”ã™ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã€‚
    """
    # å¹´
    m = (base["ç™ºè¡Œå¹´_num"].isna()) | ((base["ç™ºè¡Œå¹´_num"] >= y_from) & (base["ç™ºè¡Œå¹´_num"] <= y_to))

    # å¯¾è±¡ç‰©
    if targets_sel:
        t_norm = [_norm_lc(x) for x in targets_sel]
        m &= base["targets_lc"].apply(lambda s: any(t in s for t in t_norm))

    # ç ”ç©¶ã‚¿ã‚¤ãƒ—
    if types_sel:
        tt_norm = [_norm_lc(x) for x in types_sel]
        m &= base["types_lc"].apply(lambda s: any(t in s for t in tt_norm))

    for toks in base.loc[m, "tokens"]:
        yield toks


# =========================================================
# â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
# =========================================================

def _render_freq_block(df: pd.DataFrame, key_ns: str = "kw_freq") -> None:
    st.markdown("### ğŸ”¤ é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
    ymin, ymax = _year_bounds(df)

    # ---- æ¡ä»¶UIï¼ˆã‚­ãƒ¼ã¯é‡è¤‡é˜²æ­¢ã§åå‰ç©ºé–“ã‚’ä»˜ã‘ã‚‹ï¼‰
    c1, c2, c3 = st.columns([1, 1, 1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax,
                                 value=(ymin, ymax), key=f"{key_ns}_year")
    with c2:
        targets_q = st.text_input("å¯¾è±¡ç‰©ï¼ˆéƒ¨åˆ†ä¸€è‡´ãƒ»ã‚«ãƒ³ãƒ/ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", key=f"{key_ns}_tgt")
        targets_sel = [w.strip() for w in _SPLIT_RE.split(targets_q) if w.strip()]
    with c3:
        types_q = st.text_input("ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆéƒ¨åˆ†ä¸€è‡´ãƒ»ã‚«ãƒ³ãƒ/ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", key=f"{key_ns}_typ")
        types_sel = [w.strip() for w in _SPLIT_RE.split(types_q) if w.strip()]

    c4, c5 = st.columns([1, 1])
    with c4:
        top_n = st.number_input("ä¸Šä½N", min_value=10, max_value=200, value=30, step=10, key=f"{key_ns}_topn")
    with c5:
        do_wc = st.toggle("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚‚å‡ºã™ï¼ˆä»»æ„ï¼‰", value=False, key=f"{key_ns}_wc")

    # ---- å‰å‡¦ç†ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦å–å¾—
    kw_cols = tuple([c for c in KW_COLS_DEFAULT if c in df.columns])
    base = precompute_keywords_df(df, kw_cols=kw_cols)

    # ---- é›†è¨ˆï¼ˆè»½ã„ï¼‰
    cnt = Counter()
    for toks in _apply_filters(base, y_from, y_to, targets_sel, types_sel):
        cnt.update(toks)

    if not cnt:
        st.info("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    items = cnt.most_common(int(top_n))
    freq_df = pd.DataFrame(items, columns=["keyword", "count"])

    # è¡¨
    st.dataframe(freq_df, use_container_width=True, hide_index=True)

    # ãƒãƒ¼
    st.bar_chart(freq_df.set_index("keyword")["count"])

    # ä»»æ„: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
    if do_wc:
        if HAS_WC and HAS_MPL:
            wc = WordCloud(width=1024, height=512, background_color="white",
                           font_path=None).generate_from_frequencies(dict(cnt))
            fig = plt.figure(figsize=(10, 5))
            plt.imshow(wc, interpolation="bilinear")
            plt.axis("off")
            st.pyplot(fig, clear_figure=True)
        else:
            st.info("WordCloud / matplotlib ãŒæœªå°å…¥ã®ãŸã‚è¡¨ç¤ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")


# =========================================================
# â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
# =========================================================

def _render_cooccurrence_block(df: pd.DataFrame, key_ns: str = "kw_cooc") -> None:
    st.markdown("### ğŸ”— å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆè«–æ–‡å†…ã§ä¸€ç·’ã«å‡ºã‚‹èªï¼‰")

    ymin, ymax = _year_bounds(df)

    c0, c1, c2, c3 = st.columns([1, 1, 1, 1])
    with c0:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax,
                                 value=(ymin, ymax), key=f"{key_ns}_year")
    with c1:
        targets_q = st.text_input("å¯¾è±¡ç‰©ï¼ˆéƒ¨åˆ†ä¸€è‡´ãƒ»ã‚«ãƒ³ãƒ/ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", key=f"{key_ns}_tgt")
        targets_sel = [w.strip() for w in _SPLIT_RE.split(targets_q) if w.strip()]
    with c2:
        types_q = st.text_input("ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆéƒ¨åˆ†ä¸€è‡´ãƒ»ã‚«ãƒ³ãƒ/ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", key=f"{key_ns}_typ")
        types_sel = [w.strip() for w in _SPLIT_RE.split(types_q) if w.strip()]
    with c3:
        min_w = st.number_input("å…±èµ·å›æ•°ã®ä¸‹é™ wâ‰¥", min_value=1, max_value=50, value=2, step=1, key=f"{key_ns}_minw")

    c4, c5 = st.columns([1, 1])
    with c4:
        max_kw_per_doc = st.number_input("1è«–æ–‡ã‚ãŸã‚Šæœ€å¤§èªæ•°", min_value=5, max_value=50, value=15, step=5, key=f"{key_ns}_maxk")
    with c5:
        show_top = st.number_input("ä¸Šä½ã‚¨ãƒƒã‚¸æ•°ï¼ˆè¡¨ï¼‰", min_value=20, max_value=500, value=100, step=20, key=f"{key_ns}_topE")

    kw_cols = tuple([c for c in KW_COLS_DEFAULT if c in df.columns])
    base = precompute_keywords_df(df, kw_cols=kw_cols)

    # ã‚¨ãƒƒã‚¸é›†è¨ˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¸è¦ï¼šãƒ•ã‚£ãƒ«ã‚¿ä¾å­˜ã®ãŸã‚è»½ãå®Ÿè¡Œï¼‰
    pair_cnt = Counter()
    for toks in _apply_filters(base, y_from, y_to, targets_sel, types_sel):
        if not toks:
            continue
        toks_uni = list(dict.fromkeys(toks))[: int(max_kw_per_doc)]  # é‡è¤‡é™¤å» + ä¸Šé™
        for s, t in itertools.combinations(sorted(toks_uni), 2):
            pair_cnt[(s, t)] += 1

    # ä¸‹é™ã§ã‚«ãƒƒãƒˆ
    edges = [(a, b, w) for (a, b), w in pair_cnt.items() if w >= int(min_w)]
    if not edges:
        st.info("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹å…±èµ·ã‚¨ãƒƒã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    edges = sorted(edges, key=lambda x: x[2], reverse=True)
    edge_df = pd.DataFrame(edges[: int(show_top)], columns=["src", "dst", "weight"])
    st.dataframe(edge_df, use_container_width=True, hide_index=True)

    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆä»»æ„ï¼‰
    with st.expander("ğŸ•¸ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯è¦–åŒ–ï¼ˆnetworkx / pyvis ä»»æ„ï¼‰", expanded=False):
        if HAS_NX and HAS_PYVIS:
            draw = st.button("ğŸŒ å¯è¦–åŒ–ã™ã‚‹", key=f"{key_ns}_draw")
            if draw:
                # Graph æ§‹ç¯‰
                G = nx.Graph()
                for s, t, w in edges:
                    G.add_edge(s, t, weight=int(w))
                # PyVis
                net = Network(height="700px", width="100%", bgcolor="#fff", font_color="#222")
                net.barnes_hut(gravity=-30000, central_gravity=0.25, spring_length=110)
                for n in G.nodes():
                    net.add_node(n, label=n)
                for s, t, d in G.edges(data=True):
                    w = int(d.get("weight", 1))
                    net.add_edge(s, t, value=w, title=f"å…±èµ·: {w}")

                # ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•ã‚ªãƒ¼ãƒ—ãƒ³ã‚’é¿ã‘ã€åŸ‹ã‚è¾¼ã¿ç”¨HTMLã‚’ç”Ÿæˆ
                html = net.generate_html(notebook=False)
                st.components.v1.html(html, height=720, scrolling=True)
        else:
            st.info("networkx / pyvis ãŒæœªå°å…¥ã®ãŸã‚ã€è¡¨ã®ã¿è¡¨ç¤ºã—ã¾ã™ã€‚")


# =========================================================
# â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆå¹´åˆ¥ï¼‰
# =========================================================

def _render_trend_block(df: pd.DataFrame, key_ns: str = "kw_trend") -> None:
    st.markdown("### ğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆå¹´åˆ¥å‡ºç¾é »åº¦ï¼‰")

    ymin, ymax = _year_bounds(df)
    c1, c2, c3 = st.columns([1, 1, 1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax,
                                 value=(max(ymin, ymax-20), ymax), key=f"{key_ns}_year")
    with c2:
        targets_q = st.text_input("å¯¾è±¡ç‰©ï¼ˆéƒ¨åˆ†ä¸€è‡´ãƒ»ã‚«ãƒ³ãƒ/ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", key=f"{key_ns}_tgt")
        targets_sel = [w.strip() for w in _SPLIT_RE.split(targets_q) if w.strip()]
    with c3:
        types_q = st.text_input("ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆéƒ¨åˆ†ä¸€è‡´ãƒ»ã‚«ãƒ³ãƒ/ç©ºç™½åŒºåˆ‡ã‚Šï¼‰", key=f"{key_ns}_typ")
        types_sel = [w.strip() for w in _SPLIT_RE.split(types_q) if w.strip()]

    c4, c5 = st.columns([1, 1])
    with c4:
        top_n = st.number_input("Top N èª", min_value=5, max_value=50, value=15, step=5, key=f"{key_ns}_topn")
    with c5:
        min_year_hits = st.number_input("å°‘ãªãã¨ã‚‚å‡ºç¾ã™ã‚‹å¹´æ•°ï¼ˆãƒã‚¤ã‚ºæŠ‘åˆ¶ï¼‰", min_value=1, max_value=10, value=2, step=1, key=f"{key_ns}_minyr")

    kw_cols = tuple([c for c in KW_COLS_DEFAULT if c in df.columns])
    base = precompute_keywords_df(df, kw_cols=kw_cols)

    # ã¾ãšã¯å¯¾è±¡ç¯„å›²ã§é »å‡ºTopNèªã‚’é¸ã¶
    total_cnt = Counter()
    per_year = defaultdict(Counter)  # year -> Counter(keyword -> count)
    for toks in _apply_filters(base, y_from, y_to, targets_sel, types_sel):
        # å¹´ã®å–å¾—ã¯ base ã‹ã‚‰ã§ããªã„ã®ã§ã€å†è¨ˆç®—ç”¨ã«ç™ºè¡Œå¹´ã‚’åŒã˜æ¡ä»¶ã§å–ã‚‹å¿…è¦ã‚ã‚Š
        # â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥DFã®è¡Œã‚¹ãƒ©ã‚¤ã‚¹ãŒåˆ†ã‹ã‚‰ãªã„ãŸã‚ã€ç°¡æ˜“ã« year ã‚’æ¨å®šï¼šNoneã¯ã‚¹ã‚­ãƒƒãƒ—
        #   ã“ã“ã§ã¯ precompute ã—ãŸ DF ã‚’å†åº¦æ¡ä»¶å¼ã§çµã‚‹ã®ãŒæœ€ã‚‚æ­£ç¢º
        pass

    # æ­£ç¢ºã«å¹´åˆ¥é›†è¨ˆã™ã‚‹ã«ã¯ã€é©ç”¨å¯¾è±¡ã®è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã®ã§
    # ã“ã“ã ã‘ã¯ base ã‚’ç›´æ¥ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦æ˜ç¤ºçš„ã«å›ã™
    m = (base["ç™ºè¡Œå¹´_num"].isna()) | ((base["ç™ºè¡Œå¹´_num"] >= y_from) & (base["ç™ºè¡Œå¹´_num"] <= y_to))
    if targets_sel:
        t_norm = [_norm_lc(x) for x in targets_sel]
        m &= base["targets_lc"].apply(lambda s: any(t in s for t in t_norm))
    if types_sel:
        tt_norm = [_norm_lc(x) for x in types_sel]
        m &= base["types_lc"].apply(lambda s: any(t in s for t in tt_norm))

    sub = base.loc[m]
    if sub.empty:
        st.info("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    for _, r in sub.iterrows():
        year = r["ç™ºè¡Œå¹´_num"]
        if pd.isna(year):
            continue
        year = int(year)
        toks = r["tokens"]
        total_cnt.update(toks)
        per_year[year].update(toks)

    # TopèªæŠ½å‡ºï¼ˆå°‘ãªãã¨ã‚‚ min_year_hits å¹´ä»¥ä¸Šã«å‡ºç¾ï¼‰
    years_with_kw = Counter()
    for y, c in per_year.items():
        for k in c.keys():
            years_with_kw[k] += 1

    candidates = [k for k, v in total_cnt.most_common() if years_with_kw[k] >= int(min_year_hits)]
    top_keys = candidates[: int(top_n)]
    if not top_keys:
        st.info("æ¡ä»¶ã«åˆã†ä¸Šä½èªãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚é–¾å€¤ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        return

    # æŠ˜ã‚Œç·šç”¨ãƒ‡ãƒ¼ã‚¿
    line_df = []
    for y in sorted(per_year.keys()):
        c = per_year[y]
        for k in top_keys:
            line_df.append({"year": y, "keyword": k, "count": c.get(k, 0)})
    line_df = pd.DataFrame(line_df)

    st.dataframe(
        line_df.pivot(index="year", columns="keyword", values="count").fillna(0).astype(int),
        use_container_width=True
    )

    # Streamlitã®line_chartã§ç°¡æ½”ã«
    st.line_chart(
        line_df.pivot(index="year", columns="keyword", values="count").fillna(0),
        use_container_width=True
    )


# =========================================================
# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚Œã‚‹æç”»é–¢æ•°
# =========================================================

def render_keyword_tab(df: pd.DataFrame) -> None:
    """
    åˆ†æ>ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ã‚¿ãƒ–ã®ãƒ¡ã‚¤ãƒ³ã€‚å†…éƒ¨ã§ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ®µè½åŒ–ã€‚
    ãã‚Œãã‚Œã®ãƒ–ãƒ­ãƒƒã‚¯ã¯ç‹¬ç«‹ã«ã‚­ãƒ¼ç©ºé–“ã‚’æŒã¡ã€ç›¸äº’ã«å¹²æ¸‰ã—ãªã„ã€‚
    """
    st.header("ğŸ§  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ")

    with st.expander("â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", expanded=True):
        _render_freq_block(df, key_ns="kw_freq")

    with st.expander("â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", expanded=False):
        _render_cooccurrence_block(df, key_ns="kw_cooc")

    with st.expander("â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆå¹´åˆ¥æ¨ç§»ï¼‰", expanded=False):
        _render_trend_block(df, key_ns="kw_trend")