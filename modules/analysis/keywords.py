# modules/analysis/keywords.py
# -*- coding: utf-8 -*-
"""
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã‚¿ãƒ–ï¼š
â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æï¼ˆå¹´/å¯¾è±¡ç‰©/ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚£ãƒ«ã‚¿ â†’ ä¸Šä½ã‚’å¯è¦–åŒ–ï¼‰
â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆåŒä¸€è«–æ–‡å†…ã§å…±èµ·ã™ã‚‹èªã‚’ã‚¨ãƒƒã‚¸åŒ–ï¼‰
â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆå¹´ã”ã¨ã®å‡ºç¾é »åº¦ â†—ï¸/â†˜ï¸ ã‚’å¯è¦–åŒ–ï¼‰

ä¾å­˜ã¯ã™ã¹ã¦ä»»æ„æ‰±ã„ã«ã—ã¦ã„ã¾ã™ï¼ˆç„¡ã‘ã‚Œã°è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ï¼š
- plotlyï¼ˆæ£’/æŠ˜ã‚Œç·šï¼‰â†’ ç„¡ã„å ´åˆã¯ Streamlit ã®ç°¡æ˜“ãƒãƒ£ãƒ¼ãƒˆ
- wordcloudï¼ˆãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ï¼‰â†’ ç„¡ã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
- networkx & pyvisï¼ˆå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰â†’ ç„¡ã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
"""

from __future__ import annotations

import re
import itertools
from collections import Counter, defaultdict
from typing import Iterable, List, Tuple, Dict

import pandas as pd
import streamlit as st

# --- optional deps (ç„¡ã‘ã‚Œã°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯) ---
try:
    import plotly.express as px  # type: ignore
    HAS_PLOTLY = True
except Exception:
    HAS_PLOTLY = False

try:
    from wordcloud import WordCloud  # type: ignore
    HAS_WORDCLOUD = True
except Exception:
    HAS_WORDCLOUD = False

try:
    import networkx as nx  # type: ignore
    from pyvis.network import Network  # type: ignore
    HAS_GRAPH = True
except Exception:
    HAS_GRAPH = False


# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
_SPLIT_RE = re.compile(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+")

def split_multi(cell) -> List[str]:
    """è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒåŒºåˆ‡ã‚Šã§å…¥ã£ã¦ã„ã‚‹ã‚»ãƒ«ã‚’åˆ†å‰²"""
    if cell is None:
        return []
    return [w.strip() for w in _SPLIT_RE.split(str(cell)) if w.strip()]

def norm_token(s: str) -> str:
    s = str(s or "")
    s = s.replace("\u00A0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s.lower()

@st.cache_data(ttl=600, show_spinner=False)
def collect_keywords(df: pd.DataFrame) -> List[str]:
    """
    DFã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã‚’è‡ªå‹•çš„ã«æ¤œå‡ºã—ã¦ã€å€™è£œåˆ—åã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    """
    cols = [str(c).strip() for c in df.columns]
    preferred = [
        "llm_keywords", "primary_keywords", "secondary_keywords", "featured_keywords",
        "keywords", "keyword"
    ]
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1ã€œ10ç³»ã‚‚æ‹¾ã†
    preferred += [c for c in cols if re.fullmatch(r"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\d+", c)]
    # æ—¢çŸ¥ã®åˆ—ã ã‘æ®‹ã™ï¼ˆé †åºä¿æŒï¼‰
    return [c for c in preferred if c in cols]

def concat_keywords_in_row(row: pd.Series, kw_cols: List[str]) -> List[str]:
    """1ãƒ¬ã‚³ãƒ¼ãƒ‰å†…ã®è¤‡æ•°åˆ—ã«æ•£ã‚‰ã°ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã¾ã¨ã‚ã¦ãƒˆãƒ¼ã‚¯ãƒ³é…åˆ—ã«"""
    toks: List[str] = []
    for c in kw_cols:
        toks += split_multi(row.get(c, ""))
    # é‡è¤‡é™¤å»ï¼ˆèªå½¢ã¯ãã®ã¾ã¾ã€æ¯”è¼ƒã¯lowerï¼‰
    seen = set()
    out: List[str] = []
    for t in toks:
        k = norm_token(t)
        if not k or k in seen:
            continue
        seen.add(k)
        out.append(t)   # â€œè¡¨è¨˜â€ã¯å…ƒã®ã¾ã¾ä¿æŒ
    return out

def filter_df(df: pd.DataFrame,
              year_from: int, year_to: int,
              targets: list[str], types: list[str]) -> pd.DataFrame:
    """å¹´ãƒ»å¯¾è±¡ç‰©ãƒ»ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚£ãƒ«ã‚¿"""
    use = df.copy()

    if "ç™ºè¡Œå¹´" in use.columns:
        y = pd.to_numeric(use["ç™ºè¡Œå¹´"], errors="coerce")
        use = use[(y >= year_from) & (y <= year_to) | y.isna()]

    if targets and "å¯¾è±¡ç‰©_top3" in use.columns:
        ts = [norm_token(t) for t in targets]
        use = use[use["å¯¾è±¡ç‰©_top3"].astype(str).apply(lambda v: any(t in norm_token(v) for t in ts))]

    if types and "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in use.columns:
        tp = [norm_token(t) for t in types]
        use = use[use["ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3"].astype(str).apply(lambda v: any(t in norm_token(v) for t in tp))]

    return use

def make_year_min_max(df: pd.DataFrame) -> Tuple[int, int]:
    if "ç™ºè¡Œå¹´" in df.columns:
        y = pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce")
        if y.notna().any():
            return int(y.min()), int(y.max())
    return 1980, 2025


# ========= â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ =========
def _render_freq_block(df: pd.DataFrame):
    st.markdown("### â‘  é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")

    kw_cols = collect_keywords(df)
    if not kw_cols:
        st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ—åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    # ãƒ•ã‚£ãƒ«ã‚¿UI
    ymin, ymax = make_year_min_max(df)
    c1, c2, c3 = st.columns([1.2, 1.2, 1.2])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax, value=(ymin, ymax))
    with c2:
        target_candidates = sorted({t for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)})
        targets = st.multiselect("å¯¾è±¡ç‰©", target_candidates, default=[])
    with c3:
        type_candidates = sorted({t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)})
        types = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—", type_candidates, default=[])

    use = filter_df(df, y_from, y_to, targets, types)

    # é›†è¨ˆ
    counter = Counter()
    surface_form = {}  # lower -> è¡¨è¨˜ï¼ˆæœ€åˆã«å‡ºãŸã‚‚ã®ï¼‰
    for _, row in use.iterrows():
        toks = concat_keywords_in_row(row, kw_cols)
        for t in toks:
            k = norm_token(t)
            counter[k] += 1
            surface_form.setdefault(k, t)

    if not counter:
        st.info("æ¡ä»¶ã«åˆã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    topn = st.slider("ä¸Šä½è¡¨ç¤ºä»¶æ•°", 5, 100, 30, 5)
    items = counter.most_common(topn)
    freq_df = pd.DataFrame([{"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": surface_form[k], "å‡ºç¾æ•°": n} for k, n in items])

    # æ£’ã‚°ãƒ©ãƒ•
    if HAS_PLOTLY:
        fig = px.bar(freq_df.sort_values("å‡ºç¾æ•°", ascending=True),
                     x="å‡ºç¾æ•°", y="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", orientation="h",
                     title="é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸Šä½ï¼‰")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.dataframe(freq_df, use_container_width=True, hide_index=True)

    # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆä»»æ„ï¼‰
    with st.expander("ğŸ§© ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆä»»æ„ï¼‰", expanded=False):
        if not HAS_WORDCLOUD:
            st.info("wordcloud ãŒæœªå°å…¥ã®ãŸã‚è¡¨ç¤ºã—ã¾ã›ã‚“ã€‚")
        else:
            wc = WordCloud(width=900, height=500, background_color="white",
                           colormap=None, prefer_horizontal=0.9,
                           regexp=r"[^ \n]+").generate_from_frequencies(
                {surface_form[k]: v for k, v in dict(counter).items()}
            )
            st.image(wc.to_array(), caption="WordCloud", use_column_width=True)


# ========= â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ =========
def _render_cooccurrence_block(df: pd.DataFrame):
    st.markdown("### â‘¡ å…±èµ·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")

    kw_cols = collect_keywords(df)
    if not kw_cols:
        st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ—åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    # ãƒ•ã‚£ãƒ«ã‚¿UI
    ymin, ymax = make_year_min_max(df)
    c1, c2, c3, c4 = st.columns([1.2, 1.2, 1.0, 1.0])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax, value=(ymin, ymax), key="kw2_year")
    with c2:
        target_candidates = sorted({t for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)})
        targets = st.multiselect("å¯¾è±¡ç‰©", target_candidates, default=[], key="kw2_target")
    with c3:
        type_candidates = sorted({t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)})
        types = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—", type_candidates, default=[], key="kw2_type")
    with c4:
        min_w = st.number_input("è¡¨ç¤ºã™ã‚‹æœ€å°å…±èµ·å›æ•° (wâ‰¥)", 1, 20, 2)

    use = filter_df(df, y_from, y_to, targets, types)

    # ã‚¨ãƒƒã‚¸æŠ½å‡º
    pair_counter: Dict[Tuple[str, str], int] = Counter()
    label_form: Dict[str, str] = {}  # lower -> original
    for _, row in use.iterrows():
        toks = concat_keywords_in_row(row, kw_cols)
        # lowerã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
        lower_unique = {}
        for t in toks:
            k = norm_token(t)
            if k not in lower_unique:
                lower_unique[k] = t
                label_form.setdefault(k, t)
        lows = sorted(lower_unique.keys())
        for a, b in itertools.combinations(lows, 2):
            pair_counter[(a, b)] += 1

    if not pair_counter:
        st.info("å…±èµ·é–¢ä¿‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # ä¸Šä½è¡¨ç¤º
    edges = [(label_form[a], label_form[b], w) for (a, b), w in pair_counter.items()]
    edges_df = pd.DataFrame(edges, columns=["source", "target", "weight"])
    st.dataframe(edges_df.sort_values("weight", ascending=False).head(50),
                 use_container_width=True, hide_index=True)

    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æç”»ï¼ˆä»»æ„ï¼‰
    with st.expander("ğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æç”»ï¼ˆä»»æ„ï¼šnetworkx / pyvisï¼‰", expanded=False):
        if not HAS_GRAPH:
            st.info("networkx / pyvis ãŒæœªå°å…¥ã®ãŸã‚æç”»ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return

        # PyVisæç”»
        G = nx.Graph()
        for s, t, w in edges:
            if w < int(min_w):
                continue
            if G.has_edge(s, t):
                G[s][t]["weight"] += int(w)
            else:
                G.add_edge(s, t, weight=int(w))

        if G.number_of_edges() == 0:
            st.warning("æŒ‡å®šã—ãŸæœ€å°å…±èµ·å›æ•°ä»¥ä¸Šã®ã‚¨ãƒƒã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return

        net = Network(height="700px", width="100%", bgcolor="#ffffff", font_color="#222")
        net.barnes_hut(central_gravity=0.25, spring_length=110, spring_strength=0.02)
        for n in G.nodes():
            net.add_node(n, label=n)
        for s, t, d in G.edges(data=True):
            w = int(d.get("weight", 1))
            net.add_edge(s, t, value=w, title=f"å…±èµ·å›æ•°: {w}")

        # ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•ã‚ªãƒ¼ãƒ—ãƒ³ã‚’é¿ã‘ã‚‹ï¼šHTMLç”Ÿæˆã—ã¦åŸ‹ã‚è¾¼ã¿
        html = net.generate_html(notebook=False)
        st.components.v1.html(html, height=700, scrolling=True)


# ========= â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆçµŒå¹´å¤‰åŒ–ï¼‰ =========
def _render_trend_block(df: pd.DataFrame):
    st.markdown("### â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆçµŒå¹´å¤‰åŒ–ï¼‰")

    kw_cols = collect_keywords(df)
    if not kw_cols:
        st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ—åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    ymin, ymax = make_year_min_max(df)
    c1, c2, c3, c4 = st.columns([1.2, 1.2, 1.0, 1.0])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax, value=(ymin, ymax), key="kw3_year")
    with c2:
        target_candidates = sorted({t for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)})
        targets = st.multiselect("å¯¾è±¡ç‰©", target_candidates, default=[], key="kw3_target")
    with c3:
        type_candidates = sorted({t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)})
        types = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—", type_candidates, default=[], key="kw3_type")
    with c4:
        top_n = st.number_input("ä¸Šä½Nèªï¼ˆå…¨æœŸé–“é »åº¦ã§æŠ½å‡ºï¼‰", min_value=3, max_value=30, value=10, step=1)

    use = filter_df(df, y_from, y_to, targets, types)

    if "ç™ºè¡Œå¹´" not in use.columns:
        st.info("ç™ºè¡Œå¹´åˆ—ãŒç„¡ã„ãŸã‚ã€æ™‚ç³»åˆ—ã‚’æç”»ã§ãã¾ã›ã‚“ã€‚")
        return

    # å…¨æœŸé–“ã§ä¸Šä½Nèªï¼ˆlowerã§é›†è¨ˆï¼‰
    total_counter = Counter()
    surface_form = {}
    for _, row in use.iterrows():
        toks = concat_keywords_in_row(row, kw_cols)
        for t in toks:
            k = norm_token(t)
            total_counter[k] += 1
            surface_form.setdefault(k, t)

    if not total_counter:
        st.info("æ¡ä»¶ã«åˆã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    top_keys = [k for k, _ in total_counter.most_common(int(top_n))]
    keep_labels = {k: surface_form[k] for k in top_keys}

    # å¹´Ã—èªã§ã‚«ã‚¦ãƒ³ãƒˆ
    rows = []
    yser = pd.to_numeric(use["ç™ºè¡Œå¹´"], errors="coerce")
    for (_, r), y in zip(use.iterrows(), yser):
        if pd.isna(y):
            continue
        toks = [norm_token(t) for t in concat_keywords_in_row(r, kw_cols)]
        present = set(toks)
        for k in top_keys:
            if k in present:
                rows.append((int(y), keep_labels[k]))

    if not rows:
        st.info("é¸æŠç¯„å›²å†…ã§ä¸Šä½èªã®å‡ºç¾ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    trend = pd.DataFrame(rows, columns=["year", "keyword"]).value_counts().reset_index()
    trend.columns = ["year", "keyword", "count"]

    if HAS_PLOTLY:
        fig = px.line(trend.sort_values(["keyword", "year"]),
                      x="year", y="count", color="keyword",
                      markers=True,
                      title="ä¸Šä½ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å¹´åˆ¥å‡ºç¾é »åº¦")
        fig.update_layout(xaxis=dict(dtick=1))
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.dataframe(trend.sort_values(["keyword", "year"]),
                     use_container_width=True, hide_index=True)


# ========= ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–¢æ•° =========
def render_keyword_tab(df: pd.DataFrame) -> None:
    st.markdown("## ğŸ§  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ")

    info_cols = []
    if "ç™ºè¡Œå¹´" in df.columns: info_cols.append("ç™ºè¡Œå¹´")
    if "å¯¾è±¡ç‰©_top3" in df.columns: info_cols.append("å¯¾è±¡ç‰©_top3")
    if "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in df.columns: info_cols.append("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3")
    if info_cols:
        st.caption("åˆ©ç”¨åˆ—: " + " / ".join(info_cols))

    with st.expander("ä½¿ã„æ–¹", expanded=False):
        st.markdown(
            "- ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå¹´ãƒ»å¯¾è±¡ç‰©ãƒ»ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼‰ã§çµã£ã¦ã‹ã‚‰ã€å„åˆ†æã®ä¸Šä½èªã‚„ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç¢ºèªã—ã¾ã™ã€‚\n"
            "- å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ **networkx + pyvis** ãŒå…¥ã£ã¦ã„ã‚Œã°ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚\n"
            "- Plotly ãŒç„¡ã„ç’°å¢ƒã§ã‚‚æœ€ä½é™ã®è¡¨ã¯è¦‹ã‚‰ã‚Œã‚‹ã‚ˆã†ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚"
        )

    # â‘  é »å‡º
    _render_freq_block(df)
    st.divider()

    # â‘¡ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    _render_cooccurrence_block(df)
    st.divider()

    # â‘¢ ãƒˆãƒ¬ãƒ³ãƒ‰
    _render_trend_block(df)