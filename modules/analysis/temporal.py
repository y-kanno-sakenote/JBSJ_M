# modules/analysis/temporal.py
# -*- coding: utf-8 -*-
"""
çµŒå¹´çš„å¤‰åŒ–ï¼šä¸­å¿ƒæ€§ã®æ™‚ç³»åˆ—ï¼ˆç§»å‹•çª“ï¼‰å¯è¦–åŒ–
- å¹´/å¯¾è±¡ç‰©/ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚£ãƒ«ã‚¿
- metric: degree / betweenness / eigenvector
- PlotlyãŒç„¡ã„ç’°å¢ƒã§ã‚‚è¡¨ã¯è¡¨ç¤ºï¼ˆä»»æ„ä¾å­˜ï¼‰
- ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼ˆ.cacheé…ä¸‹ï¼‰
"""

from __future__ import annotations
from pathlib import Path
import re
import itertools
import pandas as pd
import streamlit as st

# Optional deps
try:
    import networkx as nx
    HAS_NX = True
except Exception:
    HAS_NX = False

try:
    import plotly.express as px
    HAS_PX = True
except Exception:
    HAS_PX = False

# å…±æœ‰
def split_authors(cell):
    if cell is None: return []
    return [w.strip() for w in re.split(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ]+", str(cell)) if w.strip()]
def split_multi(s):
    if not s: return []
    return [w.strip() for w in re.split(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+", str(s)) if w.strip()]

# ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥
CACHE_DIR = Path(".cache"); CACHE_DIR.mkdir(exist_ok=True)
def _sig(*parts) -> str:
    import hashlib
    h = hashlib.md5()
    for p in parts: h.update(str(p).encode("utf-8"))
    return h.hexdigest()
def _cache_csv(prefix: str, *params) -> Path:
    return CACHE_DIR / f"{prefix}_{_sig(*params)}.csv"
def _load_csv(p: Path) -> pd.DataFrame | None:
    if p.exists():
        try: return pd.read_csv(p)
        except Exception: return None
    return None
def _save_csv(df: pd.DataFrame, p: Path):
    try: df.to_csv(p, index=False)
    except Exception: pass

# ========== ã‚³ã‚¢è¨ˆç®— ==========
def _apply_filters(df: pd.DataFrame,
                   year_from: int | None, year_to: int | None,
                   targets_sel: list[str] | None, types_sel: list[str] | None) -> pd.DataFrame:
    use = df.copy()
    if "ç™ºè¡Œå¹´" in use.columns and year_from is not None and year_to is not None:
        y = pd.to_numeric(use["ç™ºè¡Œå¹´"], errors="coerce")
        use = use[(y >= year_from) & (y <= year_to) | y.isna()]
    if targets_sel and "å¯¾è±¡ç‰©_top3" in use.columns:
        keys = [k.lower() for k in targets_sel]
        use = use[use["å¯¾è±¡ç‰©_top3"].astype(str).str.lower().apply(lambda v: any(k in v for k in keys))]
    if types_sel and "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in use.columns:
        keys = [k.lower() for k in types_sel]
        use = use[use["ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3"].astype(str).str.lower().apply(lambda v: any(k in v for k in keys))]
    return use

def _edges_from_df(df: pd.DataFrame) -> pd.DataFrame:
    rows = []
    for authors in df.get("è‘—è€…", pd.Series(dtype=str)).fillna(""):
        names = split_authors(authors)
        u = sorted(set(names))
        for s, t in itertools.combinations(u, 2):
            rows.append((s, t))
    if not rows:
        return pd.DataFrame(columns=["src","dst","weight"])
    edges = pd.DataFrame(rows, columns=["src","dst"])
    edges["pair"] = edges.apply(lambda r: tuple(sorted([r["src"], r["dst"]])), axis=1)
    edges = edges.groupby("pair").size().reset_index(name="weight")
    edges[["src","dst"]] = pd.DataFrame(edges["pair"].tolist(), index=edges.index)
    return edges.drop(columns=["pair"])

def _centrality_score(edges: pd.DataFrame, metric: str) -> pd.Series:
    """return Series(author -> score). networkxç„¡ã„æ™‚ã¯å…±è‘—æ•°åˆè¨ˆã§ä»£æ›¿ã€‚"""
    if edges.empty:
        return pd.Series(dtype=float)

    # ä»£æ›¿ï¼ˆnetworkxãªã—ï¼‰
    if not HAS_NX:
        deg = pd.concat([
            edges.groupby("src")["weight"].sum(),
            edges.groupby("dst")["weight"].sum(),
        ], axis=1).fillna(0)
        return deg.sum(axis=1).rename("score")

    # networkx ã‚ã‚Š
    G = nx.Graph()
    for _, r in edges.iterrows():
        s, t, w = r["src"], r["dst"], float(r["weight"])
        if G.has_edge(s, t):
            G[s][t]["weight"] += w
        else:
            G.add_edge(s, t, weight=w)
    if len(G) == 0:
        return pd.Series(dtype=float)

    if metric == "betweenness":
        cen = nx.betweenness_centrality(G, weight="weight", normalized=True)
    elif metric == "eigenvector":
        try:
            cen = nx.eigenvector_centrality_numpy(G, weight="weight")
        except Exception:
            cen = nx.degree_centrality(G)
    else:
        cen = nx.degree_centrality(G)
    return pd.Series(cen, name="score")

def _compute_centrality_over_windows(df: pd.DataFrame, metric: str, window: int) -> pd.DataFrame:
    """
    éé‡è¤‡ã®ç§»å‹•çª“ï¼ˆwindowå¹´ï¼‰ã§ä¸­å¿ƒæ€§ã‚’ç®—å‡ºã€‚
    return: tidy DF ['year_start','year_end','author','score']
    """
    if "ç™ºè¡Œå¹´" not in df.columns:
        return pd.DataFrame(columns=["year_start","year_end","author","score"])
    y = pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce").dropna().astype(int)
    if y.empty:
        return pd.DataFrame(columns=["year_start","year_end","author","score"])
    ymin, ymax = int(y.min()), int(y.max())
    rows = []
    # éé‡è¤‡ï¼ˆä¾‹: 2000-2004, 2005-2009 ...ï¼‰
    for ys in range(ymin, ymax+1, window):
        ye = min(ys + window - 1, ymax)
        dwin = df[(pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce").between(ys, ye))]
        edges = _edges_from_df(dwin)
        score = _centrality_score(edges, metric)
        if score.empty: 
            continue
        for author, val in score.items():
            rows.append((ys, ye, author, float(val)))
    out = pd.DataFrame(rows, columns=["year_start","year_end","author","score"])
    return out

def get_temporal_centrality(df: pd.DataFrame, metric: str, window: int,
                            year_from: int | None, year_to: int | None,
                            targets_sel: list[str] | None, types_sel: list[str] | None,
                            use_disk_cache: bool) -> pd.DataFrame:
    use = _apply_filters(df, year_from, year_to, targets_sel, types_sel)
    p = _cache_csv("temporal_centrality",
                   metric, window, len(use), year_from, year_to,
                   ",".join(sorted(targets_sel or [])),
                   ",".join(sorted(types_sel or [])))
    if use_disk_cache:
        cached = _load_csv(p)
        if cached is not None: return cached

    out = _compute_centrality_over_windows(use, metric=metric, window=window)
    if use_disk_cache and not out.empty:
        _save_csv(out, p)
    return out

# ========== UI ==========
def render_temporal_tab(df: pd.DataFrame, use_disk_cache: bool = False):
    st.markdown("## â³ ç ”ç©¶è€…ã®â€œä¸­å¿ƒåº¦â€ã®çµŒå¹´å¤‰åŒ–")

    # å¹´ç¯„å›²
    if "ç™ºè¡Œå¹´" in df.columns:
        y = pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce")
        if y.notna().any():
            ymin, ymax = int(y.min()), int(y.max())
        else:
            ymin, ymax = 1980, 2025
    else:
        ymin, ymax = 1980, 2025

    kpref = "tmp_"
    c1, c2, c3, c4 = st.columns([1,1,1,1])
    with c1:
        year_from, year_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin, max_value=ymax,
                                       value=(ymin, ymax), key=f"{kpref}yr")
    with c2:
        metric = st.selectbox("ä¸­å¿ƒæ€§ã®ç¨®é¡", ["degree", "betweenness", "eigenvector"], index=0, key=f"{kpref}met",
                              help="networkxæœªå°å…¥æ™‚ã¯å…±è‘—æ•°åˆè¨ˆã‚’ä»£æ›¿ã‚¹ã‚³ã‚¢ã¨ã—ã¦åˆ©ç”¨")
    with c3:
        window = st.selectbox("çª“å¹…ï¼ˆå¹´ï¼‰", [3,5,10], index=1, key=f"{kpref}win")
    with c4:
        top_k = st.number_input("å¯è¦–åŒ–ã™ã‚‹ä¸Šä½äººæ•°", min_value=3, max_value=50, value=10, step=1, key=f"{kpref}k")

    c5, c6 = st.columns([1,1])
    with c5:
        targets_all = sorted({t for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)})
        targets_sel = st.multiselect("å¯¾è±¡ç‰©ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", targets_all, default=[], key=f"{kpref}tg")
    with c6:
        types_all = sorted({t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)})
        types_sel = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", types_all, default=[], key=f"{kpref}tp")

    # è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰
    tidy = get_temporal_centrality(df, metric=metric, window=int(window),
                                   year_from=year_from, year_to=year_to,
                                   targets_sel=targets_sel, types_sel=types_sel,
                                   use_disk_cache=use_disk_cache)
    if tidy.empty:
        st.info("å¯¾è±¡æœŸé–“ãƒ»æ¡ä»¶ã§æœ‰åŠ¹ãªå…±è‘—ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ¡ä»¶ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        return

    # æœŸé–“ã®ä»£è¡¨å¹´ï¼ˆä¸­å¿ƒï¼‰ã‚’xè»¸ç”¨ã«
    tidy["year_mid"] = (tidy["year_start"] + tidy["year_end"]) / 2

    # ä¸Šä½è‘—è€…é¸å®šï¼ˆå…¨æœŸé–“å¹³å‡ã‚¹ã‚³ã‚¢ï¼‰
    top_authors = (tidy.groupby("author")["score"].mean()
                   .sort_values(ascending=False).head(int(top_k)).index.tolist())
    sub = tidy[tidy["author"].isin(top_authors)].copy()

    st.markdown("### ğŸ“ˆ ä¸­å¿ƒæ€§ã®æ¨ç§»ï¼ˆä¸Šä½ï¼‰")
    if HAS_PX:
        fig = px.line(sub, x="year_mid", y="score", color="author",
                      markers=True, labels={"year_mid":"å¹´","score":"ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢","author":"è‘—è€…"},
                      title="ç ”ç©¶è€…ã®ä¸­å¿ƒæ€§æ¨ç§»ï¼ˆç§»å‹•çª“ï¼‰")
        fig.update_layout(legend_title_text="è‘—è€…", hovermode="x unified")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.dataframe(sub.pivot_table(index="year_mid", columns="author", values="score")
                     .sort_index(), use_container_width=True)

    with st.expander("çµæœãƒ‡ãƒ¼ã‚¿ï¼ˆtidyï¼‰", expanded=False):
        st.dataframe(sub[["year_start","year_end","author","score"]]
                     .sort_values(["author","year_start"]), use_container_width=True, hide_index=True)