# modules/analysis/temporal.py
# -*- coding: utf-8 -*-
"""
â³ çµŒå¹´å¤‰åŒ–ï¼ˆä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢ã®ç§»å‹•çª“ãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰

ã“ã®ã‚¿ãƒ–ã¯ã€ç‰¹å®šæœŸé–“ã®å…±è‘—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰ã€Œç ”ç©¶è€…ã®ä¸­å¿ƒæ€§ã€ãŒ
æ™‚é–“ã¨ã¨ã‚‚ã«ã©ã†ç§»ã‚Šå¤‰ã‚ã£ãŸã‹ã‚’ã€ç§»å‹•çª“ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ã™ã‚‹å¹´åŒºé–“ï¼‰ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚

â–  ã§ãã‚‹ã“ã¨
- ãƒ•ã‚£ãƒ«ã‚¿ï¼šå¯¾è±¡ç‰©ãƒ»ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§è«–æ–‡é›†åˆã‚’çµã‚Šè¾¼ã¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
- æŒ‡æ¨™é¸æŠï¼šdegree / betweenness / eigenvector ã‚’åˆ‡ã‚Šæ›¿ãˆ
- æœŸé–“è¨­å®šï¼šã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å¹…ï¼ˆå¹´ï¼‰ãƒ»ã‚·ãƒ•ãƒˆå¹…ï¼ˆå¹´ï¼‰ãƒ»é–‹å§‹å¹´ã§ç§»å‹•çª“ã‚’å®šç¾©
- å¯è¦–åŒ–ï¼šå„ç§»å‹•çª“ã§ç®—å‡ºã—ãŸä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢ã®æ¨ç§»ã‚’æŠ˜ã‚Œç·šè¡¨ç¤º
- å®Ÿå‹™è£œåŠ©ï¼šç›´è¿‘ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆè‘—è€… / å…±è‘—æ•° / ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢ï¼‰ã‚’ç¢ºèª

â–  ç”¨èªã–ã£ãã‚Š
- æ¬¡æ•°ä¸­å¿ƒæ€§ï¼ˆdegreeï¼‰ï¼šã©ã‚Œã ã‘å¤šãã®ç›¸æ‰‹ã¨ã¤ãªãŒã£ã¦ã„ã‚‹ã‹ï¼ˆæ¨ªã®åºƒã•ï¼‰
- åª’ä»‹ä¸­å¿ƒæ€§ï¼ˆbetweennessï¼‰ï¼šç ”ç©¶è€…åŒå£«ã®æ©‹æ¸¡ã—ã®åº¦åˆã„ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¦ï¼‰
- å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ä¸­å¿ƒæ€§ï¼ˆeigenvectorï¼‰ï¼šå½±éŸ¿åŠ›ã®ã‚ã‚‹ç›¸æ‰‹ã¨ã¤ãªãŒã£ã¦ã„ã‚‹ã»ã©é«˜ã„ï¼ˆå½±éŸ¿ã®è³ªï¼‰
  â€» networkx ãŒæœªå°å…¥ã®å ´åˆã¯ã€è¿‘ä¼¼ã¨ã—ã¦ã€Œå…±è‘—æ•°ã®åˆè¨ˆã€ã‚’ã‚¹ã‚³ã‚¢ã«ä½¿ã„ã¾ã™ã€‚

â–  è¡¨ç¤ºã®èª­ã¿æ–¹
- æŠ˜ã‚Œç·š1æœ¬ï¼1äººã®ç ”ç©¶è€…ã€‚ãƒ©ã‚¤ãƒ³ãŒä¸Šæ˜‡ã™ã‚Œã°ãã®æœŸé–“ã§å½±éŸ¿åŠ›ãŒå¢—åŠ ã€‚
- ãƒ•ã‚£ãƒ«ã‚¿ã‚’ä½¿ã†ã¨ã€ç‰¹å®šé ˜åŸŸï¼ˆä¾‹ï¼šæ¸…é…’Ã—å¾®ç”Ÿç‰©ï¼‰ã ã‘ã®â€œãƒªãƒ¼ãƒ€ãƒ¼äº¤ä»£â€ãŒè¦‹ãˆã¾ã™ã€‚
"""

from __future__ import annotations
import itertools
import re
import pandas as pd
import streamlit as st

# ---- Optional depsï¼ˆç„¡ã‘ã‚Œã°è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰----
try:
    import networkx as nx
    HAS_NX = True
except Exception:
    HAS_NX = False

try:
    import plotly.express as px
    HAS_PLOTLY = True
except Exception:
    HAS_PLOTLY = False


# ========= æ–‡å­—åˆ—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def _split_authors(cell) -> list[str]:
    """è‘—è€…ã‚»ãƒ«ã‚’åŒºåˆ‡ã‚Šè¨˜å·ã§åˆ†å‰²ã€‚ç©ºè¦ç´ ã¯é™¤å»ã€‚"""
    if cell is None:
        return []
    return [w.strip() for w in re.split(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ]+", str(cell)) if w.strip()]

def _split_multi(s):
    """'æ¸…é…’; ãƒ¯ã‚¤ãƒ³ / ãƒ“ãƒ¼ãƒ«' ã®ã‚ˆã†ãªè¤‡åˆæ–‡å­—åˆ—ã‚’åˆ†å‰²ã€‚"""
    if not s:
        return []
    return [w.strip() for w in re.split(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+", str(s)) if w.strip()]

def _norm_key(s: str) -> str:
    """å°æ–‡å­—åŒ–ï¼‹å…¨è§’/é€£ç¶šç©ºç™½ã®æ­£è¦åŒ–ï¼ˆéƒ¨åˆ†ä¸€è‡´ç”¨ï¼‰ã€‚"""
    s = str(s or "")
    s = s.replace("\u00A0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s.lower()

def _col_contains_any(df_col: pd.Series, needles: list[str]) -> pd.Series:
    """åˆ—ã«å¯¾ã—ã¦ needles ã®ã„ãšã‚Œã‹ãŒéƒ¨åˆ†ä¸€è‡´ã™ã‚‹ã‹ï¼ˆæ­£è¦åŒ–ã—ã¦è©•ä¾¡ï¼‰ã€‚"""
    if not needles:
        return pd.Series([True] * len(df_col), index=df_col.index)
    lo_needles = [_norm_key(n) for n in needles]
    def _hit(v: str) -> bool:
        s = _norm_key(v)
        return any(n in s for n in lo_needles)
    return df_col.fillna("").astype(str).map(_hit)


# ========= å¹´ãƒ¬ãƒ³ã‚¸ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def _year_bounds(df: pd.DataFrame) -> tuple[int, int]:
    """DFã‹ã‚‰ç™ºè¡Œå¹´ã®æœ€å°/æœ€å¤§ã‚’å–å¾—ã€‚ç„¡ã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã€‚"""
    if "ç™ºè¡Œå¹´" in df.columns:
        y = pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce")
        if y.notna().any():
            return int(y.min()), int(y.max())
    return 1980, 2025


# ========= ã‚¨ãƒƒã‚¸æ§‹ç¯‰ï¼ˆç§»å‹•çª“ã§ä½¿ã„å›ã™ã®ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ =========
@st.cache_data(ttl=600, show_spinner=False)
def _build_edges(df: pd.DataFrame, y_from: int, y_to: int) -> pd.DataFrame:
    """
    [y_from, y_to] ã®ç¯„å›²ã§å…±è‘—ã‚¨ãƒƒã‚¸ã‚’æ§‹ç¯‰ã€‚
    è¿”ã‚Šå€¤: ['src','dst','weight']
    """
    use = df.copy()

    # å¹´ãƒ¬ãƒ³ã‚¸ã§çµã‚Šè¾¼ã¿ï¼ˆæ¬ æå¹´ã¯é€šã™ï¼ãƒ¬ãƒ“ãƒ¥ãƒ¼ç­‰ã‚’æ®‹ã™ï¼‰
    if "ç™ºè¡Œå¹´" in use.columns:
        y = pd.to_numeric(use["ç™ºè¡Œå¹´"], errors="coerce")
        use = use[(y >= y_from) & (y <= y_to) | y.isna()]

    # è‘—è€…ãƒšã‚¢ã‚’é‡ã¿ä»˜ãã§ã‚«ã‚¦ãƒ³ãƒˆ
    rows = []
    for authors in use.get("è‘—è€…", pd.Series(dtype=str)).fillna(""):
        names = sorted(set(_split_authors(authors)))
        for s, t in itertools.combinations(names, 2):
            rows.append((s, t))

    if not rows:
        return pd.DataFrame(columns=["src", "dst", "weight"])

    edges = pd.DataFrame(rows, columns=["src", "dst"])
    edges["pair"] = edges.apply(lambda r: tuple(sorted([r["src"], r["dst"]])), axis=1)
    edges = edges.groupby("pair").size().reset_index(name="weight")
    edges[["src", "dst"]] = pd.DataFrame(edges["pair"].tolist(), index=edges.index)
    edges = edges.drop(columns=["pair"]).sort_values("weight", ascending=False).reset_index(drop=True)
    return edges[["src", "dst", "weight"]]


# ========= ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢ï¼ˆnetworkx ç„¡ã—ã§ã‚‚å‹•ãï¼‰ =========
def _centrality_from_edges(edges: pd.DataFrame, metric: str = "degree") -> pd.Series:
    """
    ã‚¨ãƒƒã‚¸â†’ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢ï¼ˆSeries: index=author, value=scoreï¼‰
    - networkx ç„¡ã—ï¼šé‡ã¿ä»˜ãæ¬¡æ•°ï¼ˆå…±è‘—é‡ã¿ã®åˆè¨ˆï¼‰ã§è¿‘ä¼¼
    """
    if edges.empty:
        return pd.Series(dtype=float)

    if not HAS_NX:
        deg = (
            pd.concat(
                [edges.groupby("src")["weight"].sum(), edges.groupby("dst")["weight"].sum()],
                axis=1,
            )
            .fillna(0)
            .sum(axis=1)
            .sort_values(ascending=False)
        )
        deg.name = "score"
        return deg

    # networkx ã‚ã‚Šï¼šæœ¬æ ¼è¨ˆç®—
    G = nx.Graph()
    for _, r in edges.iterrows():
        s, t, w = r["src"], r["dst"], float(r["weight"])
        if G.has_edge(s, t):
            G[s][t]["weight"] += w
        else:
            G.add_edge(s, t, weight=w)

    if metric == "betweenness":
        cen = nx.betweenness_centrality(G, weight="weight", normalized=True)
    elif metric == "eigenvector":
        try:
            cen = nx.eigenvector_centrality_numpy(G, weight="weight")
        except Exception:
            cen = nx.degree_centrality(G)  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    else:
        cen = nx.degree_centrality(G)

    s = pd.Series(cen, dtype=float).sort_values(ascending=False)
    s.name = "score"
    return s


# ========= æ™‚ç³»åˆ—ï¼ˆç§»å‹•çª“ï¼‰ã‚¹ã‚³ã‚¢ =========
@st.cache_data(ttl=600, show_spinner=False)
def _sliding_window_scores(
    df: pd.DataFrame,
    metric: str,
    start_year: int,
    win: int,
    step: int,
    ymax: int,
) -> pd.DataFrame:
    """
    start_year ã‹ã‚‰ win å¹´ã®çª“ã‚’ step å¹´ãšã¤å³ã¸ã‚¹ãƒ©ã‚¤ãƒ‰ã—ãªãŒã‚‰ä¸­å¿ƒæ€§ã‚’ç®—å‡ºã€‚
    è¿”ã‚Šå€¤: longå½¢å¼ ['window','author','score']ï¼ˆwindow ã¯ "YYYY-YYYY" æ–‡å­—åˆ—ï¼‰
    """
    records = []
    s = start_year
    while s <= ymax - win + 1:
        e = s + win - 1
        edges = _build_edges(df, s, e)
        scores = _centrality_from_edges(edges, metric=metric)
        if not scores.empty:
            rec = pd.DataFrame(
                {"window": f"{s}-{e}", "author": scores.index, "score": scores.values}
            )
            records.append(rec)
        s += step

    if not records:
        return pd.DataFrame(columns=["window", "author", "score"])
    return pd.concat(records, ignore_index=True)


# ========= ãƒ¡ã‚¤ãƒ³æç”» =========
def render_temporal_tab(df: pd.DataFrame, use_disk_cache: bool = True) -> None:
    """
    UIã®æµã‚Œï¼š
      1) ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå¯¾è±¡ç‰© / ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼‰
      2) æœŸé–“ã¨æŒ‡æ¨™ã®è¨­å®šï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å¹…ãƒ»ã‚·ãƒ•ãƒˆå¹…ãƒ»é–‹å§‹å¹´ãƒ»ä¸Šä½è‘—è€…æ•°ï¼‰
      3) æ™‚ç³»åˆ—ã‚¹ã‚³ã‚¢ã‚’ç®—å‡º â†’ æŠ˜ã‚Œç·šã§æ¨ç§»ã‚’è¡¨ç¤º
      4) ç›´è¿‘ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚‚ä½µè¨˜ï¼ˆå®Ÿå‹™ã§ã®ç¢ºèªç”¨ï¼‰
    """
    st.markdown("## â³ ç ”ç©¶ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®çµŒå¹´å¤‰åŒ–ï¼ˆç§»å‹•çª“ï¼‰")

    if df is None or "è‘—è€…" not in df.columns:
        st.info("è‘—è€…ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # ---- å¹´ç¯„å›²ã®è‡ªå‹•æ¨å®šï¼ˆå®‰å…¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ ----
    ymin, ymax = _year_bounds(df)

    # ---- 1) å¯¾è±¡ç‰©/ç ”ç©¶ã‚¿ã‚¤ãƒ— ã§è»½é‡ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰----
    st.markdown("### ğŸ” çµã‚Šè¾¼ã¿æ¡ä»¶ï¼ˆä»»æ„ï¼‰")
    c_f1, c_f2 = st.columns(2)
    with c_f1:
        tg_raw = {t for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("") for t in _split_multi(v)}
        targets_sel = st.multiselect("å¯¾è±¡ç‰©", sorted(tg_raw), default=[], key="temporal_tg")
    with c_f2:
        tp_raw = {t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("") for t in _split_multi(v)}
        types_sel = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—", sorted(tp_raw), default=[], key="temporal_tp")

    df_filt = df.copy()
    if targets_sel and "å¯¾è±¡ç‰©_top3" in df_filt.columns:
        df_filt = df_filt[_col_contains_any(df_filt["å¯¾è±¡ç‰©_top3"], targets_sel)]
    if types_sel and "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in df_filt.columns:
        df_filt = df_filt[_col_contains_any(df_filt["ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3"], types_sel)]

    if df_filt.empty:
        st.warning("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        return

    # ---- 2) ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦è¨­å®š + æŒ‡æ¨™é¸æŠ ----
    st.markdown("### âš™ï¸ æœŸé–“ã¨æŒ‡æ¨™ã®è¨­å®š")
    c1, c2, c3, c4, c5 = st.columns([1, 1, 1, 1, 1])
    with c1:
        metric = st.selectbox(
            "ä¸­å¿ƒæ€§æŒ‡æ¨™",
            ["degree", "betweenness", "eigenvector"],
            index=0,
            key="temporal_metric",
            help="networkx æœªå°å…¥æ™‚ã¯â€œå…±è‘—æ•°ã®åˆè¨ˆâ€ã§ä»£æ›¿ã—ã¾ã™ã€‚",
            format_func=lambda x: {"degree": "æ¬¡æ•°", "betweenness": "åª’ä»‹", "eigenvector": "å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«"}[x],
        )
    with c2:
        win = st.number_input(
            "ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å¹…ï¼ˆå¹´ï¼‰",
            min_value=2,
            max_value=max(2, ymax - ymin + 1),
            value=min(5, max(2, ymax - ymin + 1)),
            step=1,
            key="temporal_win",
            help="1ã¤ã®çª“ã®å¹´æ•°ã€‚ä¾‹ï¼š5å¹´ãªã‚‰â€œ2000â€“2004â€ã§1åŒºé–“ã€‚",
        )
    with c3:
        step = st.number_input(
            "ã‚·ãƒ•ãƒˆå¹…ï¼ˆå¹´ï¼‰",
            min_value=1,
            max_value=max(1, ymax - ymin + 1),
            value=1,
            step=1,
            key="temporal_step",
            help="çª“ã‚’ã©ã‚Œã ã‘å³ã¸é€²ã‚ã‚‹ã‹ã€‚1ãªã‚‰2000â€“2004 â†’ 2001â€“2005 â†’ â€¦",
        )
    max_start = max(ymin, ymax - int(win) + 1)
    with c4:
        start_year = st.slider(
            "é–‹å§‹å¹´",
            min_value=ymin,
            max_value=max_start,
            value=min(ymin, max_start),
            step=1,
            key="temporal_start",
            help="æœ€åˆã®çª“ã®å·¦ç«¯ã€‚ã“ã“ã‹ã‚‰ã‚·ãƒ•ãƒˆå¹…ãšã¤å³ã¸è©•ä¾¡ã—ã¾ã™ã€‚",
        )
    with c5:
        top_k = st.number_input(
            "ä¸Šä½è‘—è€…æ•°",
            min_value=3,
            max_value=30,
            value=10,
            step=1,
            key="temporal_topk",
            help="å¯è¦–åŒ–å¯¾è±¡ã®è‘—è€…æ•°ï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å…¨ä½“ã§é‡è¦åº¦ã®é«˜ã„é †ï¼‰ã€‚",
        )

    end_year = start_year + int(win) - 1
    st.caption(f"ğŸ“… å¯¾è±¡æœŸé–“: **{start_year}â€“{end_year}**ï¼ˆ{win}å¹´ãƒ»ã‚·ãƒ•ãƒˆ {step}å¹´ï¼‰")

    # ---- 3) ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰ã‚Šï¼‰----
    with st.spinner("æ™‚ç³»åˆ—ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ä¸­..."):
        # â€» ç§»å‹•çª“ã®å†…éƒ¨ã¯ _build_edges / _centrality_from_edges ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿
        scores_long = _sliding_window_scores(
            df=df_filt,
            metric=metric,
            start_year=start_year,
            win=int(win),
            step=int(step),
            ymax=ymax,
        )

    if scores_long.empty:
        st.info("è©²å½“æœŸé–“ã§å…±è‘—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒæ§‹ç¯‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å¹´ç¯„å›²ã‚„ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        return

    # å¯è¦–åŒ–å¯¾è±¡ã®è‘—è€…ã‚’ä¸Šä½ã«çµã‚‹ï¼ˆå…¨æœŸé–“ã®æœ€å¤§ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼‰
    top_authors = (
        scores_long.groupby("author")["score"]
        .max()
        .sort_values(ascending=False)
        .head(int(top_k))
        .index
        .tolist()
    )
    plot_df = scores_long[scores_long["author"].isin(top_authors)].copy()

    # ---- 4) æŠ˜ã‚Œç·šå¯è¦–åŒ– ----
    st.markdown("### ğŸ“ˆ ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢ã®æ¨ç§»ï¼ˆç§»å‹•çª“ï¼‰")
    if HAS_PLOTLY:
        fig = px.line(
            plot_df, x="window", y="score", color="author", markers=True, template="plotly_white",
            labels={"window": "ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆå¹´åŒºé–“ï¼‰", "score": "ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢", "author": "è‘—è€…"},
        )
        fig.update_layout(legend_title_text="è‘—è€…", height=460, margin=dict(l=10, r=10, t=10, b=10))
        st.plotly_chart(fig, use_container_width=True)
    else:
        pivot = plot_df.pivot(index="window", columns="author", values="score").fillna(0.0)
        st.line_chart(pivot)

    with st.expander("ğŸ“„ ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º", expanded=False):
        st.dataframe(plot_df.sort_values(["window", "score"], ascending=[True, False]), hide_index=True)

    # ---- 5) ç›´è¿‘ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆç¾æ³ç¢ºèªï¼‰----
    st.markdown("### ğŸ” ç›´è¿‘ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ä¸Šä½")
    last_from = max(end_year - int(win) + 1, start_year)
    last_to = end_year
    edges_last = _build_edges(df_filt, last_from, last_to)
    # ç›´è¿‘ã¯è¡¨ã‚’è¦‹ã‚„ã™ãï¼ˆå…±è‘—æ•°ã‚‚ä½µè¨˜ï¼‰
    # å…±è‘—æ•°ï¼é‡ã¿åˆè¨ˆï¼ˆnetworkxç„¡ã§ã‚‚è¨ˆç®—å¯ï¼‰
    deg_last = (
        pd.concat(
            [edges_last.groupby("src")["weight"].sum(), edges_last.groupby("dst")["weight"].sum()],
            axis=1,
        )
        .fillna(0)
        .sum(axis=1)
        .rename("coauth_count")
        .reset_index()
        .rename(columns={"index": "author"})
    )
    scores_last = _centrality_from_edges(edges_last, metric=metric).rename("score").reset_index().rename(columns={"index": "author"})
    rank_last = pd.merge(scores_last, deg_last, on="author", how="left").fillna({"coauth_count": 0})
    rank_last = rank_last.sort_values("score", ascending=False).head(30)
    rank_last = rank_last.rename(columns={"author": "è‘—è€…", "score": "ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢", "coauth_count": "å…±è‘—æ•°"})
    st.dataframe(rank_last, use_container_width=True, hide_index=True)

    st.caption("â€» æŒ‡æ¨™ã®æ„å‘³ï¼šæ¬¡æ•°=ã¤ãªãŒã‚Šã®æ•° / åª’ä»‹=æ©‹æ¸¡ã—åº¦ / å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«=å½±éŸ¿åŠ›ï¼ˆå½±éŸ¿åŠ›ã®é«˜ã„ç›¸æ‰‹ã¨ã®çµã³ä»˜ãï¼‰")