# modules/analysis/coauthor_temporal.py
# -*- coding: utf-8 -*-
"""
å…±è‘—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®çµŒå¹´å¤‰åŒ–ï¼ˆã‚µãƒ–ã‚¿ãƒ–ç”¨ãƒ»temporal.pyæº–æ‹ ï¼‰
- å¹´ãƒ¬ãƒ³ã‚¸ãƒ»å¯¾è±¡ç‰©ãƒ»ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚£ãƒ«ã‚¿
- ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦å¹…ã¨ã‚¹ãƒ†ãƒƒãƒ—ã§æœŸé–“ã‚’ã‚¹ãƒ©ã‚¤ãƒ‰ â†’ ä¸­å¿ƒæ€§ï¼ˆæ¬¡æ•°/åª’ä»‹/å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã‚’ç®—å‡º
- ä¸Šä½ç ”ç©¶è€…ã®ã‚¹ã‚³ã‚¢æ¨ç§»ã‚’æŠ˜ã‚Œç·šã§å¯è¦–åŒ–ï¼ˆPlotly ãŒç„¡ã‘ã‚Œã° st.line_chart ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
â€» æ©Ÿèƒ½ãƒ»UIã¯æ—¢å­˜ã®ã¾ã¾ã€‚å¯¾è±¡ç‰©/ç ”ç©¶ã‚¿ã‚¤ãƒ—ã®é¸æŠè‚¢ã®ä¸¦ã³é †ã®ã¿æŒ‡å®šé †ã«æ•´åˆ—ã€‚
"""

from __future__ import annotations
import re
import itertools
from typing import List, Tuple

import pandas as pd
import streamlit as st

# ---- ä¸¦ã³é †ï¼ˆæŒ‡å®šé †ï¼‰ ----
TARGET_ORDER = [
    "æ¸…é…’","ãƒ“ãƒ¼ãƒ«","ãƒ¯ã‚¤ãƒ³","ç„¼é…","ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«é£²æ–™","ç™ºé…µä¹³ãƒ»ä¹³è£½å“",
    "é†¤æ²¹","å‘³å™Œ","ç™ºé…µé£Ÿå“","è¾²ç”£ç‰©ãƒ»æœå®Ÿ","å‰¯ç”£ç‰©ãƒ»ãƒã‚¤ã‚ªãƒã‚¹",
    "é…µæ¯ãƒ»å¾®ç”Ÿç‰©","ã‚¢ãƒŸãƒé…¸ãƒ»ã‚¿ãƒ³ãƒ‘ã‚¯è³ª","ãã®ä»–"
]
TYPE_ORDER = [
    "å¾®ç”Ÿç‰©ãƒ»éºä¼å­é–¢é€£","é†¸é€ å·¥ç¨‹ãƒ»è£½é€ æŠ€è¡“","å¿œç”¨åˆ©ç”¨ãƒ»é£Ÿå“é–‹ç™º","æˆåˆ†åˆ†æãƒ»ç‰©æ€§è©•ä¾¡",
    "å“è³ªè©•ä¾¡ãƒ»å®˜èƒ½è©•ä¾¡","æ­´å²ãƒ»æ–‡åŒ–ãƒ»çµŒæ¸ˆ","å¥åº·æ©Ÿèƒ½ãƒ»æ „é¤ŠåŠ¹æœ","çµ±è¨ˆè§£æãƒ»ãƒ¢ãƒ‡ãƒ«åŒ–",
    "ç’°å¢ƒãƒ»ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£","ä¿å­˜ãƒ»å®‰å®šæ€§","ãã®ä»–ï¼ˆç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼‰"
]

def _sort_with_order(items, order):
    """æŒ‡å®šé †ã§æ•´åˆ—ï¼ˆæœªå®šç¾©ã¯æœ«å°¾ãƒ»åå‰é †ï¼‰"""
    order_map = {n: i for i, n in enumerate(order)}
    return sorted(items, key=lambda x: (order_map.get(x, len(order)), x))


# ---- Optional deps ----
try:
    import networkx as nx  # type: ignore
    HAS_NX = True
except Exception:
    HAS_NX = False

try:
    import plotly.express as px  # type: ignore
    HAS_PX = True
except Exception:
    HAS_PX = False


# ========= å…±æœ‰ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
_AUTHOR_SPLIT_RE = re.compile(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ]+")
_MULTI_SPLIT_RE  = re.compile(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+")

def split_authors(cell) -> List[str]:
    if cell is None:
        return []
    return [w.strip() for w in _AUTHOR_SPLIT_RE.split(str(cell)) if w.strip()]

def split_multi(s) -> List[str]:
    """ 'æ¸…é…’; ãƒ¯ã‚¤ãƒ³ / ãƒ“ãƒ¼ãƒ«' ãªã©ã‚’åˆ†å‰² """
    if not s:
        return []
    return [w.strip() for w in _MULTI_SPLIT_RE.split(str(s)) if w.strip()]

def norm_key(s: str) -> str:
    s = str(s or "")
    s = s.replace("\u00A0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s.lower()

def col_contains_any(df_col: pd.Series, needles: List[str]) -> pd.Series:
    """åˆ—ï¼ˆæ–‡å­—åˆ—ï¼‰ã« needles ã®ã„ãšã‚Œã‹ãŒéƒ¨åˆ†ä¸€è‡´ã™ã‚‹ã‹ï¼ˆå°æ–‡å­—/å…¨è§’ç©ºç™½æ­£è¦åŒ–ï¼‰ã€‚"""
    if not needles:
        return pd.Series([True] * len(df_col), index=df_col.index)
    lo_needles = [norm_key(n) for n in needles]
    def _hit(v: str) -> bool:
        s = norm_key(v)
        return any(n in s for n in lo_needles)
    return df_col.fillna("").astype(str).map(_hit)


# ========= å…±è‘—ã‚¨ãƒƒã‚¸ä½œæˆ =========
@st.cache_data(ttl=600, show_spinner=False)
def build_coauthor_edges(df: pd.DataFrame,
                         year_from: int, year_to: int,
                         targets: List[str] | None = None,
                         types: List[str] | None = None) -> pd.DataFrame:
    """
    å…¥åŠ›: dfï¼ˆå°‘ãªãã¨ã‚‚ 'è‘—è€…', 'ç™ºè¡Œå¹´', 'å¯¾è±¡ç‰©_top3', 'ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3' ã‚’æ¨å¥¨ï¼‰
    å‡ºåŠ›: edges DataFrame ['src', 'dst', 'weight']
    """
    use = df.copy()

    # å¹´ã§çµã‚Šè¾¼ã¿
    if "ç™ºè¡Œå¹´" in use.columns:
        y = pd.to_numeric(use["ç™ºè¡Œå¹´"], errors="coerce")
        use = use[(y >= year_from) & (y <= year_to) | y.isna()]

    # å¯¾è±¡ç‰©ãƒ•ã‚£ãƒ«ã‚¿
    if targets:
        if "å¯¾è±¡ç‰©_top3" in use.columns:
            mask_tg = col_contains_any(use["å¯¾è±¡ç‰©_top3"], targets)
            use = use[mask_tg]

    # ç ”ç©¶ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿
    if types:
        if "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in use.columns:
            mask_tp = col_contains_any(use["ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3"], types)
            use = use[mask_tp]

    # è‘—è€…ãƒšã‚¢ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    rows: List[Tuple[str, str]] = []
    for a in use.get("è‘—è€…", pd.Series(dtype=str)).fillna(""):
        names = split_authors(a)
        for s, t in itertools.combinations(sorted(set(names)), 2):
            rows.append((s, t))

    if not rows:
        return pd.DataFrame(columns=["src", "dst", "weight"])

    edges = pd.DataFrame(rows, columns=["src", "dst"])
    edges["pair"] = edges.apply(lambda r: tuple(sorted([r["src"], r["dst"]])), axis=1)
    edges = edges.groupby("pair").size().reset_index(name="weight")
    edges[["src", "dst"]] = pd.DataFrame(edges["pair"].tolist(), index=edges.index)
    edges = edges.drop(columns=["pair"]).sort_values("weight", ascending=False).reset_index(drop=True)
    return edges[["src", "dst", "weight"]]


# ========= ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢ =========
def centrality_from_edges(edges: pd.DataFrame, metric: str = "degree") -> pd.DataFrame:
    """
    edges: ['src','dst','weight']
    metric: 'degree'|'betweenness'|'eigenvector'
    è¿”ã‚Šå€¤: ['author','score','coauth_count']
    """
    if edges.empty:
        return pd.DataFrame(columns=["author", "score", "coauth_count"])

    # ç°¡æ˜“å…±è‘—æ•°ï¼ˆé‡ã¿å’Œï¼‰ã ã‘ã¯å¸¸ã«è¨ˆç®—
    deg_simple = pd.concat([
        edges.groupby("src")["weight"].sum(),
        edges.groupby("dst")["weight"].sum(),
    ], axis=1).fillna(0)
    deg_simple["coauth_count"] = deg_simple["weight"].sum(axis=1)
    deg_simple = deg_simple["coauth_count"].reset_index().rename(columns={"index": "author"})

    if not HAS_NX:
        out = deg_simple.rename(columns={"coauth_count": "score"})
        return out[["author", "score", "coauth_count"]].sort_values("score", ascending=False).reset_index(drop=True)

    # networkx ã«ã‚ˆã‚‹ä¸­å¿ƒæ€§
    G = nx.Graph()
    for _, r in edges.iterrows():
        G.add_edge(r["src"], r["dst"], weight=float(r["weight"]))

    if metric == "betweenness":
        cen = nx.betweenness_centrality(G, weight="weight", normalized=True)
    elif metric == "eigenvector":
        try:
            cen = nx.eigenvector_centrality_numpy(G, weight="weight")
        except Exception:
            cen = nx.degree_centrality(G)
    else:
        cen = nx.degree_centrality(G)

    cen_df = pd.Series(cen, name="score").reset_index().rename(columns={"index": "author"})
    out = pd.merge(cen_df, deg_simple, on="author", how="left")
    out["coauth_count"] = out["coauth_count"].fillna(0).astype(float)
    return out[["author", "score", "coauth_count"]].sort_values("score", ascending=False).reset_index(drop=True)


# ========= ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦ã‚¹ãƒ©ã‚¤ã‚¹ï¼ˆæ™‚ç³»åˆ—ï¼‰ =========
def _window_ranges(ymin: int, ymax: int, width: int, step: int) -> List[Tuple[int, int, int]]:
    """
    ä¾‹: ymin=1990, ymax=2024, width=5, step=3
    -> [(1990,1994,1992), (1993,1997,1995), ...]  â€» (from,to,center)
    """
    out = []
    y = ymin
    while y <= ymax:
        y2 = min(y + width - 1, ymax)
        center = (y + y2) // 2
        out.append((y, y2, center))
        if y2 >= ymax:
            break
        y += step
    return out


def _timeseries_scores(df: pd.DataFrame,
                       ymin: int, ymax: int,
                       width: int, step: int,
                       metric: str,
                       targets: List[str], types: List[str],
                       top_n_each: int = 10,
                       max_authors: int = 20) -> pd.DataFrame:
    """
    æœŸé–“ã‚’ã‚¹ãƒ©ã‚¤ãƒ‰ã—ãªãŒã‚‰ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºã€‚
    å¯è¦–åŒ–ç”¨ã« ['center_year','author','score'] ã‚’è¿”ã™ã€‚
    è¡¨ç¤ºå¯¾è±¡ã® author ã¯å„ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦ã®ä¸Šä½é›†åˆã‹ã‚‰æœ€å¤§ max_authors ã«åˆ¶é™ã€‚
    """
    windows = _window_ranges(ymin, ymax, width, step)
    records = []
    author_pool = []

    for yf, yt, yc in windows:
        edges = build_coauthor_edges(df, yf, yt, targets, types)
        rank = centrality_from_edges(edges, metric=metric)
        if rank.empty:
            continue
        # ãã®çª“ã®ä¸Šä½ã‹ã‚‰å€™è£œã‚’è¿½åŠ 
        author_pool.extend(rank["author"].head(top_n_each).tolist())
        for _, r in rank.iterrows():
            records.append({"center_year": yc, "author": r["author"], "score": float(r["score"])})

    if not records:
        return pd.DataFrame(columns=["center_year", "author", "score"])

    ts = pd.DataFrame(records)
    # å¯è¦–åŒ–å¯¾è±¡ author ã‚’åˆ¶é™ï¼ˆé »å‡ºä¸Šä½ï¼‰
    top_authors = ts["author"].value_counts().head(max_authors).index.tolist()
    ts = ts[ts["author"].isin(top_authors)].copy()
    ts = ts.sort_values(["author", "center_year"]).reset_index(drop=True)
    return ts


# ========= ãƒ¡ã‚¤ãƒ³æç”»ï¼ˆã‚µãƒ–ã‚¿ãƒ–ç”¨ï¼‰ =========
def render_coauthor_temporal_subtab(df: pd.DataFrame, use_disk_cache: bool = False) -> None:
    st.markdown("### â³ å…±è‘—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®çµŒå¹´å¤‰åŒ–")

    if df is None or "è‘—è€…" not in df.columns:
        st.info("è‘—è€…ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # å¹´ã®ç¯„å›²
    if "ç™ºè¡Œå¹´" in df.columns:
        y = pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce")
        if y.notna().any():
            ymin_all, ymax_all = int(y.min()), int(y.max())
        else:
            ymin_all, ymax_all = 1980, 2025
    else:
        ymin_all, ymax_all = 1980, 2025

    # 1æ®µç›®: å¹´ãƒ»ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦è¨­å®š
    c1, c2, c3, c4 = st.columns([1, 1, 1, 1])
    with c1:
        y_from, y_to = st.slider("å¯¾è±¡å¹´ï¼ˆç¯„å›²ï¼‰", min_value=ymin_all, max_value=ymax_all,
                                 value=(ymin_all, ymax_all), key="co_temporal_year")
    with c2:
        win = st.number_input("ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦å¹…ï¼ˆå¹´ï¼‰", min_value=2, max_value=15, value=5, step=1, key="co_temporal_win")
    with c3:
        step = st.number_input("ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆå¹´ï¼‰", min_value=1, max_value=10, value=2, step=1, key="co_temporal_step")
    with c4:
        metric = st.selectbox(
            "ä¸­å¿ƒæ€§æŒ‡æ¨™",
            ["degree", "betweenness", "eigenvector"],
            index=0,
            format_func=lambda x: {
                "degree": "æ¬¡æ•°ä¸­å¿ƒæ€§ï¼ˆã¤ãªãŒã‚Šã®æ•°ï¼‰",
                "betweenness": "åª’ä»‹ä¸­å¿ƒæ€§ï¼ˆæ©‹æ¸¡ã—åº¦ï¼‰",
                "eigenvector": "å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ä¸­å¿ƒæ€§ï¼ˆå½±éŸ¿åŠ›ï¼‰",
            }[x],
            key="co_temporal_metric",
            help="networkx ãŒæœªå°å…¥ã®å ´åˆã¯ç°¡æ˜“ã‚¹ã‚³ã‚¢ï¼ˆå…±è‘—æ•°ã®åˆè¨ˆï¼‰ã§ä»£æ›¿ã—ã¾ã™ã€‚",
        )

    # 2æ®µç›®: å¯¾è±¡ç‰©ãƒ»ç ”ç©¶ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆâ€» ä¸¦ã³é †ã ã‘æŒ‡å®šé †ã«å¤‰æ›´ã€‚UI/æ–‡è¨€/ã‚­ãƒ¼ã¯æ—¢å­˜ã®ã¾ã¾ï¼‰
    c5, c6 = st.columns([1, 1])
    with c5:
        tg_raw = {t for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
        # ã“ã“ã ã‘æ•´åˆ—ï¼ˆæŒ‡å®šé †ï¼‰â€”â€” æ©Ÿèƒ½ãƒ»UIã¯ä¸å¤‰
        tg_all = _sort_with_order(list(tg_raw), TARGET_ORDER)
        tg_sel = st.multiselect("å¯¾è±¡ç‰©ã§çµã‚Šè¾¼ã¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", tg_all, default=[], key="co_temporal_tg")
    with c6:
        tp_raw = {t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("") for t in split_multi(v)}
        tp_all = _sort_with_order(list(tp_raw), TYPE_ORDER)
        tp_sel = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§çµã‚Šè¾¼ã¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", tp_all, default=[], key="co_temporal_tp")

    # å®Ÿè¡Œ
    st.markdown("#### ğŸ“ˆ ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢ã®æ¨ç§»ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦ï¼‰")
    ts = _timeseries_scores(
        df=df,
        ymin=y_from, ymax=y_to,
        width=int(win), step=int(step),
        metric=metric,
        targets=tg_sel, types=tp_sel,
        top_n_each=10, max_authors=20
    )

    if ts.empty:
        st.info("æ¡ä»¶ã«åˆã†å…±è‘—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒæ§‹ç¯‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å¹´ç¯„å›²ã‚„ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        return

    # å¯è¦–åŒ–
    if HAS_PX:
        fig = px.line(
            ts, x="center_year", y="score", color="author",
            markers=True,
            labels={"center_year": "å¹´ï¼ˆã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦ä¸­å¿ƒï¼‰", "score": "ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢", "author": "è‘—è€…"},
        )
        fig.update_layout(legend_title_text="è‘—è€…", height=520, margin=dict(l=10, r=10, t=30, b=10))
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.line_chart(
            ts.pivot_table(index="center_year", columns="author", values="score", aggfunc="mean").sort_index()
        )

    # ç›´è¿‘ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå‚è€ƒï¼‰
    st.markdown("#### ğŸ” ç›´è¿‘ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦ã®ä¸Šä½")
    last_from = max(y_to - int(win) + 1, y_from)
    last_to = y_to
    edges_last = build_coauthor_edges(df, last_from, last_to, tg_sel, tp_sel)
    rank_last = centrality_from_edges(edges_last, metric=metric).head(30)
    rank_last = rank_last.rename(columns={"author": "è‘—è€…", "score": "ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢", "coauth_count": "å…±è‘—æ•°"})
    st.dataframe(rank_last, use_container_width=True, hide_index=True)

    st.caption("â€» æŒ‡æ¨™ã®æ„å‘³ï¼šæ¬¡æ•°=ã¤ãªãŒã‚Šã®æ•° / åª’ä»‹=æ©‹æ¸¡ã—åº¦ / å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«=å½±éŸ¿åŠ›ï¼ˆæœ‰åŠ›è€…ã¨ã®çµã³ä»˜ãï¼‰")