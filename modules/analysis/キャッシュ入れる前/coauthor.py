# modules/analysis/coauthor.py
# -*- coding: utf-8 -*-
"""
å…±è‘—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆç ”ç©¶è€…ã®ã¤ãªãŒã‚Šãƒ©ãƒ³ã‚­ãƒ³ã‚° + ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯è¦–åŒ– + ç ”ç©¶ã‚¯ãƒ©ã‚¹ã‚¿ï¼‰
- è¡¨: è‘—è€… / å…±è‘—æ•° / ã¤ãªãŒã‚Šã‚¹ã‚³ã‚¢ ã®3åˆ—
- PyVisã¯ generate_html() ã«ã‚ˆã‚‹åŸ‹ã‚è¾¼ã¿
- å¹´ãƒ»å¯¾è±¡ç‰©ãƒ»ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚£ãƒ«ã‚¿
- ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºï¼ˆã‚¯ãƒ©ã‚¹ã‚¿è‰²åˆ†ã‘ï¼‰ï¼† ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ä»£è¡¨ç ”ç©¶è€…ï¼ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¦ç´„
"""

import re
import itertools
import pandas as pd
import streamlit as st

# --- Optional deps ---
try:
    import networkx as nx
    HAS_NX = True
except Exception:
    HAS_NX = False

try:
    from pyvis.network import Network
    HAS_PYVIS = True
except Exception:
    HAS_PYVIS = False


# ========= åŸºæœ¬ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
_AUTHOR_SPLIT_RE = re.compile(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ]+")

def split_authors(cell):
    if cell is None:
        return []
    return [w.strip() for w in _AUTHOR_SPLIT_RE.split(str(cell)) if w.strip()]

def _norm(s: str) -> str:
    return re.sub(r"\s+", " ", str(s or "")).strip().lower()


# ========= ã‚¨ãƒƒã‚¸ç”Ÿæˆï¼ˆå¹´/å¯¾è±¡ç‰©/ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼‰=========
@st.cache_data(ttl=600, show_spinner=False)
def build_coauthor_edges(df: pd.DataFrame,
                         year_from: int | None = None,
                         year_to: int | None = None,
                         targets_sel: list[str] | None = None,
                         types_sel: list[str] | None = None) -> pd.DataFrame:
    if df is None or "è‘—è€…" not in df.columns:
        return pd.DataFrame(columns=["src", "dst", "weight"])

    use = df.copy()

    # å¹´
    if "ç™ºè¡Œå¹´" in use.columns and (year_from is not None and year_to is not None):
        y = pd.to_numeric(use["ç™ºè¡Œå¹´"], errors="coerce")
        use = use[(y >= year_from) & (y <= year_to) | y.isna()]

    # å¯¾è±¡ç‰©
    if targets_sel and "å¯¾è±¡ç‰©_top3" in use.columns:
        keys = [_norm(t) for t in targets_sel]
        use = use[use["å¯¾è±¡ç‰©_top3"].astype(str).str.lower().apply(lambda v: any(k in v for k in keys))]

    # ç ”ç©¶ã‚¿ã‚¤ãƒ—
    if types_sel and "ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3" in use.columns:
        keys = [_norm(t) for t in types_sel]
        use = use[use["ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3"].astype(str).str.lower().apply(lambda v: any(k in v for k in keys))]

    # è‘—è€…ãƒšã‚¢
    rows = []
    for authors in use["è‘—è€…"].fillna(""):
        names = split_authors(authors)
        for s, t in itertools.combinations(sorted(set(names)), 2):
            rows.append((s, t))

    if not rows:
        return pd.DataFrame(columns=["src", "dst", "weight"])

    edges = pd.DataFrame(rows, columns=["src", "dst"])
    edges["pair"] = edges.apply(lambda r: tuple(sorted([r["src"], r["dst"]])), axis=1)
    edges = edges.groupby("pair").size().reset_index(name="weight")
    edges[["src", "dst"]] = pd.DataFrame(edges["pair"].tolist(), index=edges.index)
    edges = edges.drop(columns=["pair"]).sort_values("weight", ascending=False).reset_index(drop=True)
    return edges


def _author_coauthor_counts(edges: pd.DataFrame) -> pd.DataFrame:
    if edges.empty:
        return pd.DataFrame(columns=["è‘—è€…", "å…±è‘—æ•°"])
    left = edges.groupby("src")["weight"].sum()
    right = edges.groupby("dst")["weight"].sum()
    deg = pd.concat([left, right], axis=1).fillna(0)
    deg["å…±è‘—æ•°"] = deg.sum(axis=1).astype(int)
    return deg["å…±è‘—æ•°"].reset_index().rename(columns={"index": "è‘—è€…"})


# ========= ã‚¹ã‚³ã‚¢è¨ˆç®— =========
def _centrality_from_edges(edges: pd.DataFrame, metric: str = "degree") -> pd.DataFrame:
    counts = _author_coauthor_counts(edges)
    if not HAS_NX:
        out = counts.copy()
        out["ã¤ãªãŒã‚Šã‚¹ã‚³ã‚¢"] = out["å…±è‘—æ•°"].astype(float)
        return out[["è‘—è€…", "å…±è‘—æ•°", "ã¤ãªãŒã‚Šã‚¹ã‚³ã‚¢"]]

    G = nx.Graph()
    for _, r in edges.iterrows():
        s, t, w = r["src"], r["dst"], float(r["weight"])
        if G.has_edge(s, t):
            G[s][t]["weight"] += w
        else:
            G.add_edge(s, t, weight=w)

    if metric == "betweenness":
        cen = nx.betweenness_centrality(G, weight="weight")
    elif metric == "eigenvector":
        try:
            cen = nx.eigenvector_centrality_numpy(G, weight="weight")
        except Exception:
            cen = nx.degree_centrality(G)
    else:
        cen = nx.degree_centrality(G)

    cen_df = pd.Series(cen, name="ã¤ãªãŒã‚Šã‚¹ã‚³ã‚¢").reset_index().rename(columns={"index": "è‘—è€…"})
    out = cen_df.merge(counts, on="è‘—è€…", how="left")
    out = out.sort_values("ã¤ãªãŒã‚Šã‚¹ã‚³ã‚¢", ascending=False).reset_index(drop=True)
    return out[["è‘—è€…", "å…±è‘—æ•°", "ã¤ãªãŒã‚Šã‚¹ã‚³ã‚¢"]]


# ========= ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡º & è¦ç´„ =========
def _detect_communities(edges: pd.DataFrame):
    """ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ï¼‰æ¤œå‡ºã€‚æˆ»ã‚Š: dict(author -> cluster_id), list of sets"""
    if not HAS_NX or edges.empty:
        return {}, []
    G = nx.Graph()
    for _, r in edges.iterrows():
        G.add_edge(r["src"], r["dst"], weight=int(r["weight"]))
    # Greedyãƒ¢ã‚¸ãƒ¥ãƒ©ãƒªãƒ†ã‚£ï¼ˆå°ã€œä¸­è¦æ¨¡ã§å®‰å®šï¼‰
    try:
        from networkx.algorithms.community import greedy_modularity_communities
        comms = list(greedy_modularity_communities(G, weight="weight"))
    except Exception:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ©ãƒ™ãƒ«ä¼æ’­
        from networkx.algorithms.community import asyn_lpa_communities
        comms = list(asyn_lpa_communities(G, weight="weight"))
    node2cid = {}
    for cid, s in enumerate(comms, 1):
        for n in s:
            node2cid[n] = cid
    return node2cid, comms


def _collect_keywords_for_cluster(df: pd.DataFrame, authors_in_cluster: set[str], top_k: int = 8):
    """ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¨å®šï¼šã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è‘—è€…ãŒå«ã¾ã‚Œã‚‹è«–æ–‡ã‹ã‚‰é »å‡ºèªã‚’æŠ½å‡º"""
    if df is None or not authors_in_cluster:
        return []

    # ã©ã®åˆ—ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æºã«ã™ã‚‹ã‹ï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã ã‘ï¼‰
    KEY_COLS = [
        "featured_keywords","primary_keywords","secondary_keywords","llm_keywords",
        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰4","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰5",
        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰6","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰7","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰8","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰9","ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰10",
    ]
    use_cols = [c for c in KEY_COLS if c in df.columns]

    # ã‚¯ãƒ©ã‚¹ã‚¿è‘—è€…ãŒå«ã¾ã‚Œã‚‹è¡Œã‚’æŠ½å‡º
    def row_has_author(a_str):
        return any(a in authors_in_cluster for a in split_authors(a_str))

    use = df[df["è‘—è€…"].fillna("").apply(row_has_author)].copy()
    if use.empty or not use_cols:
        return []

    bag = []
    for c in use_cols:
        for cell in use[c].fillna(""):
            for t in re.split(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+", str(cell)):
                t = t.strip()
                if t:
                    bag.append(t)

    if not bag:
        return []
    s = pd.Series(bag).value_counts().head(top_k)
    return [f"{k}({v})" for k, v in s.items()]


def _cluster_summary(df: pd.DataFrame, edges: pd.DataFrame, rank_df: pd.DataFrame, top_n_in_cluster=5):
    """ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ä»£è¡¨ç ”ç©¶è€…ï¼ˆã‚¹ã‚³ã‚¢é †ï¼‰ã¨ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¦ç´„ã‚’è¿”ã™"""
    node2cid, comms = _detect_communities(edges)
    if not node2cid:
        return pd.DataFrame(columns=["ã‚¯ãƒ©ã‚¹ã‚¿", "ä»£è¡¨ç ”ç©¶è€…ï¼ˆä¸Šä½ï¼‰", "ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"])
    # ã‚¯ãƒ©ã‚¹ã‚¿â†’è‘—è€…ãƒªã‚¹ãƒˆ
    cluster_rows = []
    for cid, members in enumerate(comms, 1):
        authors = list(members)
        # ã‚¹ã‚³ã‚¢é †ã§ä¸Šä½ã‚’æŠœç²‹
        part = rank_df[rank_df["è‘—è€…"].isin(authors)].sort_values("ã¤ãªãŒã‚Šã‚¹ã‚³ã‚¢", ascending=False)
        top_authors = "ã€".join(part["è‘—è€…"].head(top_n_in_cluster).tolist()) if not part.empty else ""
        keywords = _collect_keywords_for_cluster(df, set(authors))
        cluster_rows.append({
            "ã‚¯ãƒ©ã‚¹ã‚¿": f"C{cid}",
            "ä»£è¡¨ç ”ç©¶è€…ï¼ˆä¸Šä½ï¼‰": top_authors,
            "ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": " / ".join(keywords) if keywords else ""
        })
    return pd.DataFrame(cluster_rows)


# ========= ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æç”» =========
def _draw_network(edges: pd.DataFrame, top_nodes=None, min_weight=1, height_px=650, color_by_cluster=True):
    """å…±è‘—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’PyVisã§æç”»ï¼ˆã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£è‰²åˆ†ã‘, generate_htmlï¼‰"""
    if not (HAS_NX and HAS_PYVIS):
        st.info("âš ï¸ ã‚°ãƒ©ãƒ•æç”»ã«ã¯ networkx / pyvis ãŒå¿…è¦ã§ã™ã€‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ã¯åˆ©ç”¨ã§ãã¾ã™ã€‚")
        return

    edges_use = edges[edges["weight"] >= min_weight]
    if edges_use.empty:
        st.warning("æ¡ä»¶ã«åˆã†å…±è‘—é–¢ä¿‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    G = nx.Graph()
    for _, r in edges_use.iterrows():
        G.add_edge(r["src"], r["dst"], weight=int(r["weight"]))

    if top_nodes:
        existing = [n for n in top_nodes if n in G]
        keep = set(existing) | {nbr for n in existing for nbr in G.neighbors(n)}
        G = G.subgraph(keep).copy()

    # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£è‰²åˆ†ã‘
    node2cid, comms = _detect_communities(edges_use) if color_by_cluster else ({}, [])
    palette = [
        "#4e79a7","#f28e2b","#e15759","#76b7b2","#59a14f",
        "#edc948","#b07aa1","#ff9da7","#9c755f","#bab0ab"
    ]

    net = Network(height=f"{height_px}px", width="100%", bgcolor="#fff", font_color="#222")
    net.barnes_hut(gravity=-30000, central_gravity=0.25, spring_length=110)

    for n in G.nodes():
        if color_by_cluster and n in node2cid:
            cid = node2cid[n]
            color = palette[(cid-1) % len(palette)]
            net.add_node(n, label=n, color=color)
        else:
            net.add_node(n, label=n)

    for s, t, d in G.edges(data=True):
        w = int(d.get("weight", 1))
        net.add_edge(s, t, value=w, title=f"å…±è‘—å›æ•°: {w}")

    html = net.generate_html(notebook=False)
    st.components.v1.html(html, height=height_px, scrolling=True)


# ========= UI =========
def render_coauthor_tab(df: pd.DataFrame):
    st.markdown("## ğŸ‘¥ ç ”ç©¶è€…ã®ã¤ãªãŒã‚Šåˆ†æï¼ˆå…±è‘—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ & ã‚¯ãƒ©ã‚¹ã‚¿ï¼‰")

    if df is None or "è‘—è€…" not in df.columns:
        st.warning("è‘—è€…ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # å¹´ç¯„å›²
    if "ç™ºè¡Œå¹´" in df.columns:
        y = pd.to_numeric(df["ç™ºè¡Œå¹´"], errors="coerce")
        if y.notna().any():
            ymin, ymax = int(y.min()), int(y.max())
        else:
            ymin, ymax = 1980, 2025
    else:
        ymin, ymax = 1980, 2025

    # --- ãƒ•ã‚£ãƒ«ã‚¿UI ---
    st.caption("å¯¾è±¡å¹´ãƒ»å¯¾è±¡ç‰©ãƒ»ç ”ç©¶ã‚¿ã‚¤ãƒ—ã§çµã‚Šè¾¼ã¿ã€å…±è‘—æ§‹é€ ã¨ç ”ç©¶ã‚¯ãƒ©ã‚¹ã‚¿ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")

    c1, c2, c3 = st.columns([1.5, 1.2, 1.2])
    with c1:
        year_from, year_to = st.slider("å¯¾è±¡å¹´", min_value=ymin, max_value=ymax, value=(ymin, ymax))
    with c2:
        targets_all = sorted({t for v in df.get("å¯¾è±¡ç‰©_top3", pd.Series(dtype=str)).fillna("") for t in re.split(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+", str(v)) if t})
        targets_sel = st.multiselect("å¯¾è±¡ç‰©ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", targets_all, default=[])
    with c3:
        types_all = sorted({t for v in df.get("ç ”ç©¶ã‚¿ã‚¤ãƒ—_top3", pd.Series(dtype=str)).fillna("") for t in re.split(r"[;ï¼›,ã€ï¼Œ/ï¼|ï½œ\sã€€]+", str(v)) if t})
        types_sel = st.multiselect("ç ”ç©¶ã‚¿ã‚¤ãƒ—ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", types_all, default=[])

    c4, c5, c6 = st.columns([1, 1, 1])
    with c4:
        metric = st.selectbox("ã‚¹ã‚³ã‚¢è¨ˆç®—æ–¹å¼", ["degree", "betweenness", "eigenvector"], index=0)
    with c5:
        top_n = st.number_input("ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä»¶æ•°", min_value=5, max_value=100, value=30, step=5)
    with c6:
        min_w = st.number_input("å…±è‘—å›æ•°ã®ä¸‹é™ (wâ‰¥)", min_value=1, max_value=20, value=2, step=1)

    # --- ã‚¨ãƒƒã‚¸ä½œæˆ ---
    edges = build_coauthor_edges(df, year_from, year_to, targets_sel, types_sel)
    if edges.empty:
        st.info("å…±è‘—é–¢ä¿‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚£ãƒ«ã‚¿ã‚’ç·©ã‚ã¦å†åº¦è©¦ã—ã¦ãã ã•ã„ã€‚")
        return

    # --- ã‚¹ã‚³ã‚¢è¨ˆç®— + è¡¨ç¤º ---
    st.markdown("### ğŸ” ç ”ç©¶è€…ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
    rank = _centrality_from_edges(edges, metric=metric).head(int(top_n))
    st.dataframe(rank, use_container_width=True, hide_index=True)

    # --- ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£è¦ç´„ï¼ˆè¡¨ï¼‰ ---
    with st.expander("ğŸ§© ç ”ç©¶ã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ï¼‰è¦ç´„", expanded=True):
        summary_df = _cluster_summary(df, edges, rank_df=rank, top_n_in_cluster=5)
        if summary_df.empty:
            st.info("ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºã«ã¯ networkx ãŒå¿…è¦ã§ã™ã€‚")
        else:
            st.dataframe(summary_df, use_container_width=True, hide_index=True)

    # --- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯è¦–åŒ–ï¼ˆè‰²åˆ†ã‘ï¼‰ ---
    with st.expander("ğŸ•¸ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å¯è¦–åŒ–ã™ã‚‹ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿è‰²åˆ†ã‘ï¼‰", expanded=False):
        st.caption("å…±è‘—é–¢ä¿‚ã‚’ã‚¯ãƒ©ã‚¹ã‚¿è‰²åˆ†ã‘ã§è¡¨ç¤ºã—ã¾ã™ï¼ˆä¾å­˜: networkx / pyvisï¼‰")
        top_only = st.toggle("ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä¸Šä½ã®å‘¨è¾ºã®ã¿è¡¨ç¤ºï¼ˆè»½é‡ï¼‰", value=True)
        top_nodes = rank["è‘—è€…"].tolist() if top_only else None
        if st.button("ğŸŒ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æç”»ã™ã‚‹"):
            _draw_network(edges, top_nodes=top_nodes, min_weight=int(min_w), height_px=700, color_by_cluster=True)